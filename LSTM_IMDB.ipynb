{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jr4cugH3mZ_X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer     # pip install tokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Kjf2yM5GF4_8",
    "outputId": "9c37cfea-ad43-474b-8acf-3129b43ceeb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in our data (140000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using SQLite Table to read data.\n",
    "con = sqlite3.connect('database.sqlite') \n",
    "\n",
    "# filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "# SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000, will give top 500000 data points\n",
    "# you can change the number to any other number based on your computing power\n",
    "\n",
    "# filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000\"\"\", con) \n",
    "# for tsne assignment you can take 5k data points\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 140000\"\"\", con) \n",
    "\n",
    "# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative\n",
    "print(\"Number of data points in our data\", filtered_data.shape)\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1194
    },
    "colab_type": "code",
    "id": "eYEE6ts7GAjC",
    "outputId": "dd68009a-ed98-4364-adef-69de773518c3"
   },
   "outputs": [],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT UserId, ProductId, ProfileName, Time, Score, Text, COUNT(*)\n",
    "FROM Reviews\n",
    "GROUP BY UserId\n",
    "HAVING COUNT(*)>1\n",
    "\"\"\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "colab_type": "code",
    "id": "57N6TyKLH-Pc",
    "outputId": "1dc02095-292e-4c2d-b0b0-9c4b25012ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80668, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R115TNMSPFT9I7</td>\n",
       "      <td>B007Y59HVM</td>\n",
       "      <td>Breyton</td>\n",
       "      <td>1331510400</td>\n",
       "      <td>2</td>\n",
       "      <td>Overall its just OK when considering the price...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife has recurring extreme muscle spasms, u...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R11DNU2NBKQ23Z</td>\n",
       "      <td>B007Y59HVM</td>\n",
       "      <td>Kim Cieszykowski</td>\n",
       "      <td>1348531200</td>\n",
       "      <td>1</td>\n",
       "      <td>This coffee is horrible and unfortunately not ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#oc-R11O5J5ZVQE25C</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>Penguin Chick</td>\n",
       "      <td>1346889600</td>\n",
       "      <td>5</td>\n",
       "      <td>This will be the bottle that you grab from the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#oc-R12KPBODL2B5ZD</td>\n",
       "      <td>B007OSBE1U</td>\n",
       "      <td>Christopher P. Presta</td>\n",
       "      <td>1348617600</td>\n",
       "      <td>1</td>\n",
       "      <td>I didnt like this coffee. Instead of telling y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId   ProductId             ProfileName        Time  Score  \\\n",
       "0  #oc-R115TNMSPFT9I7  B007Y59HVM                 Breyton  1331510400      2   \n",
       "1  #oc-R11D9D7SHXIJB9  B005HG9ET0  Louis E. Emory \"hoppy\"  1342396800      5   \n",
       "2  #oc-R11DNU2NBKQ23Z  B007Y59HVM        Kim Cieszykowski  1348531200      1   \n",
       "3  #oc-R11O5J5ZVQE25C  B005HG9ET0           Penguin Chick  1346889600      5   \n",
       "4  #oc-R12KPBODL2B5ZD  B007OSBE1U   Christopher P. Presta  1348617600      1   \n",
       "\n",
       "                                                Text  COUNT(*)  \n",
       "0  Overall its just OK when considering the price...         2  \n",
       "1  My wife has recurring extreme muscle spasms, u...         3  \n",
       "2  This coffee is horrible and unfortunately not ...         2  \n",
       "3  This will be the bottle that you grab from the...         3  \n",
       "4  I didnt like this coffee. Instead of telling y...         2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(display.shape)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "CquzlqrOIYGn",
    "outputId": "593c99ca-f15e-4df7-8ac6-0e7d0950415d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80638</th>\n",
       "      <td>AZY10LLTJ71NX</td>\n",
       "      <td>B006P7E5ZI</td>\n",
       "      <td>undertheshrine \"undertheshrine\"</td>\n",
       "      <td>1334707200</td>\n",
       "      <td>5</td>\n",
       "      <td>I was recommended to try green tea extract to ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              UserId   ProductId                      ProfileName        Time  \\\n",
       "80638  AZY10LLTJ71NX  B006P7E5ZI  undertheshrine \"undertheshrine\"  1334707200   \n",
       "\n",
       "       Score                                               Text  COUNT(*)  \n",
       "80638      5  I was recommended to try green tea extract to ...         5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display[display['UserId']=='AZY10LLTJ71NX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "A_FT0dPNIeLP",
    "outputId": "55c48adc-c76a-462f-c737-7d73bafdb5cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393063"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display['COUNT(*)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXDz-oNRbzaA"
   },
   "source": [
    "<h2>#  [2] Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIrWin3Wb9zR"
   },
   "source": [
    "<h2>## [2.1] Data Cleaning: Deduplication</h2>\n",
    "\n",
    "It is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.  Following is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30HRd6EYWX1I"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99PmS-YycKnq"
   },
   "source": [
    "As it can be seen above that same user has multiple reviews with same values for HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary and Text and on doing analysis it was found that <br>\n",
    "<br> \n",
    "ProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies, 8.82-Ounce Packages (Pack of 8)<br>\n",
    "<br> \n",
    "ProductId=B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so on<br>\n",
    "\n",
    "It was inferred after analysis that reviews with same parameters other than ProductId belonged to the same product just having different flavour or quantity. Hence in order to reduce redundancy it was decided to eliminate the rows having same parameters.<br>\n",
    "\n",
    "The method used for the same was that we first sort the data according to ProductId and then just keep the first similar product review and delelte the others. for eg. in the above just the review for ProductId=B000HDL1RQ remains. This method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAO6SB5GWYDM"
   },
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luGPt2u3WYFw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118907, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgFK1kcYWYIZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.93357142857143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MEwrVSHcVXo"
   },
   "source": [
    "<b>Observation:-</b> It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-ryUbOVWYLG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwZMuwD0WYOD"
   },
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27Bkm-bmcf1q"
   },
   "source": [
    "Observation(s):\n",
    "\n",
    "1. I have keep only those values whose HelpfulnessNumerator <= HelpfulnessDenominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPFXEnKWWYRL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118905, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    99869\n",
       "0    19036\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWZWa5fIclXP"
   },
   "source": [
    "#  [3] Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioPyjaMxctgf"
   },
   "source": [
    "## [3.1].  Preprocessing Review Text\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "no9n0GjvWYW2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the movie or sound track and sing along with Carol King. This is great stuff, my whole extended family knows these songs by heart. Quality kids storytelling and music.\n",
      "==================================================\n",
      "I love chai tea and this is definitely reminiscent of that. It's got a wonderful aroma and tastes good hot or cold. I'm definitely buying it again. It also reminds me of special home-made tea I had at a NYC Indian restaurant which was fabulous. Excellent tea. Well worth the price!\n",
      "==================================================\n",
      "Purchased this product at a local store in NY and my kids and i love it. Its a quick easy meal. You can put in a toaster oven on toast for 6 min. and its ready to eat.<br />strongly recommend\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# printing some random reviews\n",
    "sent_0 = final['Text'].values[0]\n",
    "print(sent_0)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_1000 = final['Text'].values[1000]\n",
    "print(sent_1000)\n",
    "print(\"=\"*50)\n",
    "\n",
    "sent_118904 = final['Text'].values[118904]\n",
    "print(sent_118904)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpnrsIfKWYZ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the movie or sound track and sing along with Carol King. This is great stuff, my whole extended family knows these songs by heart. Quality kids storytelling and music.\n"
     ]
    }
   ],
   "source": [
    "# remove urls from text python: https://stackoverflow.com/a/40823105/4084039\n",
    "sent_0 = re.sub(r\"http\\S+\", \"\", sent_0)\n",
    "sent_1000 = re.sub(r\"http\\S+\", \"\", sent_1000)\n",
    "sent_118904 = re.sub(r\"http\\S+\", \"\", sent_118904)\n",
    "\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyPnI4gXWYdF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the movie or sound track and sing along with Carol King. This is great stuff, my whole extended family knows these songs by heart. Quality kids storytelling and music.\n",
      "==================================================\n",
      "I love chai tea and this is definitely reminiscent of that. It's got a wonderful aroma and tastes good hot or cold. I'm definitely buying it again. It also reminds me of special home-made tea I had at a NYC Indian restaurant which was fabulous. Excellent tea. Well worth the price!\n",
      "==================================================\n",
      "Purchased this product at a local store in NY and my kids and i love it. Its a quick easy meal. You can put in a toaster oven on toast for 6 min. and its ready to eat.strongly recommend\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(sent_0, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_1000, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(sent_118904, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaBm2VD4c321"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OA__vfM5c38l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchased this product at a local store in NY and my kids and i love it. Its a quick easy meal. You can put in a toaster oven on toast for 6 min. and its ready to eat.<br />strongly recommend\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sent_118904 = decontracted(sent_118904)\n",
    "print(sent_118904)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-T0cBYb8c4C9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the movie or sound track and sing along with Carol King. This is great stuff, my whole extended family knows these songs by heart. Quality kids storytelling and music.\n"
     ]
    }
   ],
   "source": [
    "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "sent_0 = re.sub(\"\\S*\\d\\S*\", \"\", sent_0).strip()\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xn0BH8uyc4GK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchased this product at a local store in NY and my kids and i love it Its a quick easy meal You can put in a toaster oven on toast for 6 min and its ready to eat br strongly recommend\n"
     ]
    }
   ],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "sent_118904 = re.sub('[^A-Za-z0-9]+', ' ', sent_118904)\n",
    "print(sent_118904)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KQNBHYYc4WP"
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90hKArEDdGbQ"
   },
   "source": [
    "Observation(s):\n",
    "\n",
    "1. Here we are having set of stopwords.\n",
    "2. Here so many unused word in the stopwords.\n",
    "3. stopwords are words which contain a dictionary of unused words.\n",
    "4. If we are removing stopwordsthen our Bag of Words is much smaller and meaningful vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8ZSvDVac4aA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 118905/118905 [01:21<00:00, 1450.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(final['Text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eW675Nvdc4TT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'purchased product local store ny kids love quick easy meal put toaster oven toast min ready eat strongly recommend'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_reviews[118904]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAH0SZQ_c4QN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final (118905, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138687</th>\n",
       "      <td>150505</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2PTSM496CF40Z</td>\n",
       "      <td>Jason A. Teeple \"Nobody made a greater mistak...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1210809600</td>\n",
       "      <td>A classic</td>\n",
       "      <td>Get the movie or sound track and sing along wi...</td>\n",
       "      <td>get movie sound track sing along carol king gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138708</th>\n",
       "      <td>150526</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3E9QZFE9KXH8J</td>\n",
       "      <td>R. Mitchell</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1129507200</td>\n",
       "      <td>awesome book poor size</td>\n",
       "      <td>This is one of the best children's books ever ...</td>\n",
       "      <td>one best children books ever written mini vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138707</th>\n",
       "      <td>150525</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2QID6VCFTY51R</td>\n",
       "      <td>Rick</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1025481600</td>\n",
       "      <td>In December it will be, my snowman's anniversa...</td>\n",
       "      <td>My daughter loves all the \"Really Rosie\" books...</td>\n",
       "      <td>daughter loves really rosie books introduced r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>witty little book makes son laugh loud recite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138705</th>\n",
       "      <td>150523</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2P4F2UO0UMP8C</td>\n",
       "      <td>Elizabeth A. Curry \"Lovely Librarian\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1096675200</td>\n",
       "      <td>MMMM chicken soup....</td>\n",
       "      <td>Summary:  A young boy describes the usefulness...</td>\n",
       "      <td>summary young boy describes usefulness chicken...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  \\\n",
       "138687  150505  0006641040  A2PTSM496CF40Z   \n",
       "138708  150526  0006641040  A3E9QZFE9KXH8J   \n",
       "138707  150525  0006641040  A2QID6VCFTY51R   \n",
       "138706  150524  0006641040   ACITT7DI6IDDL   \n",
       "138705  150523  0006641040  A2P4F2UO0UMP8C   \n",
       "\n",
       "                                             ProfileName  \\\n",
       "138687  Jason A. Teeple \"Nobody made a greater mistak...   \n",
       "138708                                       R. Mitchell   \n",
       "138707                                              Rick   \n",
       "138706                                   shari zychinski   \n",
       "138705             Elizabeth A. Curry \"Lovely Librarian\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "138687                     1                       1      1  1210809600   \n",
       "138708                    11                      18      0  1129507200   \n",
       "138707                     1                       2      1  1025481600   \n",
       "138706                     0                       0      1   939340800   \n",
       "138705                     0                       0      1  1096675200   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138687                                          A classic   \n",
       "138708                             awesome book poor size   \n",
       "138707  In December it will be, my snowman's anniversa...   \n",
       "138706                          EVERY book is educational   \n",
       "138705                              MMMM chicken soup....   \n",
       "\n",
       "                                                     Text  \\\n",
       "138687  Get the movie or sound track and sing along wi...   \n",
       "138708  This is one of the best children's books ever ...   \n",
       "138707  My daughter loves all the \"Really Rosie\" books...   \n",
       "138706  this witty little book makes my son laugh at l...   \n",
       "138705  Summary:  A young boy describes the usefulness...   \n",
       "\n",
       "                                              CleanedText  \n",
       "138687  get movie sound track sing along carol king gr...  \n",
       "138708  one best children books ever written mini vers...  \n",
       "138707  daughter loves really rosie books introduced r...  \n",
       "138706  witty little book makes son laugh loud recite ...  \n",
       "138705  summary young boy describes usefulness chicken...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding a column of CleanedText which displays the data after pre-processing of the review\n",
    "final['CleanedText']=preprocessed_reviews\n",
    "final['CleanedText']=final['CleanedText']\n",
    "#below the processed review can be seen in the CleanedText Column \n",
    "print('Shape of final',final.shape)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYb8DObodTyu"
   },
   "source": [
    "Observation(s):\n",
    "\n",
    "1. Here processed review can be seen in the CleanedText Column.\n",
    "2. We will use it for future observation.\n",
    "3. we are having 118905 matrix of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYXudx6tJWOb"
   },
   "source": [
    "# Time Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2o6pykyyJohn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12820</th>\n",
       "      <td>13986</td>\n",
       "      <td>B000FKEWRM</td>\n",
       "      <td>A2MB5XC5QP3WBW</td>\n",
       "      <td>Ricky N. \"Ricky C. Nelson\"</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1203465600</td>\n",
       "      <td>Maxwell House Filter Packs</td>\n",
       "      <td>Maxwell House has a winner with these filter p...</td>\n",
       "      <td>maxwell house winner filter packs leave home e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70380</th>\n",
       "      <td>76559</td>\n",
       "      <td>B0049ULB78</td>\n",
       "      <td>A1QI6WTAUB6L6S</td>\n",
       "      <td>MP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1342915200</td>\n",
       "      <td>Dark Magic is a great cup of coffee</td>\n",
       "      <td>I am just 1 week into owning my Kuerig B60 and...</td>\n",
       "      <td>week owning kuerig sample pack brewed coffee g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>1836</td>\n",
       "      <td>B001RVFDOO</td>\n",
       "      <td>A32G4AR56VP4YN</td>\n",
       "      <td>Michael J. Casteel</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1327622400</td>\n",
       "      <td>Awful</td>\n",
       "      <td>I was at a charity fund raising event where th...</td>\n",
       "      <td>charity fund raising event handing seemed pret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId                 ProfileName  \\\n",
       "12820  13986  B000FKEWRM  A2MB5XC5QP3WBW  Ricky N. \"Ricky C. Nelson\"   \n",
       "70380  76559  B0049ULB78  A1QI6WTAUB6L6S                          MP   \n",
       "1694    1836  B001RVFDOO  A32G4AR56VP4YN          Michael J. Casteel   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "12820                     3                       4      1  1203465600   \n",
       "70380                     0                       0      1  1342915200   \n",
       "1694                      1                      10      0  1327622400   \n",
       "\n",
       "                                   Summary  \\\n",
       "12820           Maxwell House Filter Packs   \n",
       "70380  Dark Magic is a great cup of coffee   \n",
       "1694                                 Awful   \n",
       "\n",
       "                                                    Text  \\\n",
       "12820  Maxwell House has a winner with these filter p...   \n",
       "70380  I am just 1 week into owning my Kuerig B60 and...   \n",
       "1694   I was at a charity fund raising event where th...   \n",
       "\n",
       "                                             CleanedText  \n",
       "12820  maxwell house winner filter packs leave home e...  \n",
       "70380  week owning kuerig sample pack brewed coffee g...  \n",
       "1694   charity fund raising event handing seemed pret...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Sorting data according to Time in ascending order for Time Based Splitting\n",
    "time_sorted_data = final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "# Randomly selecting 20K datapoints\n",
    "\n",
    "# We will collect different 20K rows without repetition from time_sorted_data dataframe\n",
    "final_20 = time_sorted_data.take(np.random.permutation(len(final))[:70000])\n",
    "print(final_20.shape)\n",
    "final_20.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knKkUrd1JxTZ"
   },
   "source": [
    "Observation(s):\n",
    "1. Time based splitting is better than random splitting.\n",
    "2. In time based splitting we sort our data in ascending order.\n",
    "3. In TIme based splitting we get better accuracy than random splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FahADyFxJocM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    58885\n",
       "0    11115\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample dataset \n",
    "final_20['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTvIINPrJoZX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    99869\n",
       "0    19036\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original dataset\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "765p07YLJoWE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.297795771479982"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio of positive reviews to negative reviews in Sample Dataset\n",
    "len(final_20[final_20['Score'] == 1])/len(final_20[final_20['Score'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7lxo5i2JoS7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.246322756881698"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio of positive reviews to negative reviews in Original Dataset\n",
    "len(final[final['Score'] == 1])/len(final[final['Score'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rlmxwONJoGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# final_20 datapoints which will use to train model\n",
    "X = final_20['CleanedText'].values\n",
    "print(\"shape of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPxiY67kJnyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# class label\n",
    "y = final_20['Score'].values\n",
    "print(\"shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5MLUsMZdarv"
   },
   "source": [
    "<h2>Getting the vocabulary of all the words</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J93q4ubyd9jr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words in the Vocabulary :  49863\n"
     ]
    }
   ],
   "source": [
    "##Sorting data according to Time in ascending order for Time Based Splitting\n",
    "time_sorted_data = final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "X = time_sorted_data['CleanedText'].values\n",
    "y = time_sorted_data['Score']\n",
    "\n",
    "# Finding all words in the vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "vocabulary = tokenizer.word_index()\n",
    "print('No. of words in the Vocabulary : ',len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drjtsEssd9gF"
   },
   "outputs": [],
   "source": [
    "# Storing all words in the dictionary (words as keys and index as values)\n",
    "corpus = dict()\n",
    "ind = 0\n",
    "for sent in X:\n",
    "  for word in sent.split():\n",
    "    corpus.setdefault(word,[])\n",
    "    corpus[word].append(ind)\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZMRPfaohxbq"
   },
   "source": [
    "<h2>Getting frequency for each word in the vocabulary</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhkdCyc-h2JI"
   },
   "outputs": [],
   "source": [
    "# Getting frequency for each word of vocabulary and storing it in a list\n",
    "freq = []\n",
    "for w in vocabulary:\n",
    "  freq.append(len(corpus[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSb6Uu5Kg1ZO"
   },
   "source": [
    "<h2>Getting Index for each word in the vocabulary</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVSMhYgwd9dd"
   },
   "outputs": [],
   "source": [
    "# Getting Index for each word in the vocabulary\n",
    "# Sorting frequencies in decreasing order\n",
    "inc_index =np.argsort(np.array(freq))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xf3qhKBxd9Xm"
   },
   "outputs": [],
   "source": [
    "# Allocating ranks to words of vocabulary in decreasing order of frequency and storing words in a dictionary\n",
    "word_rank = dict()\n",
    "rank = 1\n",
    "for i in inc_index:\n",
    "  word_rank[vocabulary[i]] = rank\n",
    "  rank +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjM_R3c4h-ef"
   },
   "source": [
    "<h2>Converting this data as IMDB dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOMxILI0d9RS"
   },
   "outputs": [],
   "source": [
    "# Converting full data into imdb format\n",
    "data = []\n",
    "for sent in X:\n",
    "  row = []\n",
    "  for word in sent.split():\n",
    "    if(len(word)>1):\n",
    "      row.append(word_rank[word])\n",
    "  data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000,) (56000,) (14000,) (14000,)\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# Splitting the data set into training set and test set:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into 50-50 train_data and test_data\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation(s):\n",
    "    \n",
    "1. I have split the dataset into training set and testing set.\n",
    "2. Here 80% of the data used to train the model and 20% of the data used to test its performance of the test set.\n",
    "3. Here i have 56000 points to train the model and 14000 points to test its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuxxc97ZipIZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxD4o1IuipVU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 100)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 60803  1223 22918 47584 62833 23675 44662 20734 41904 50838 45055 15407\n",
      " 33510 24621   330  2571]\n"
     ]
    }
   ],
   "source": [
    "# truncate and/or pad input sequences\n",
    "max_review_length = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POLNhgkiipeG"
   },
   "outputs": [],
   "source": [
    "# this function is used draw Binary Crossentropy Loss VS No. of epochs plot\n",
    "def plt_dynamic(x, vy, ty):\n",
    "  plt.figure(figsize=(10,5))\n",
    "  plt.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "  plt.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "  plt.xlabel('Epochs') \n",
    "  plt.ylabel('Binary Crossentropy Loss')\n",
    "  plt.title('\\nError Plot: Binary Crossentropy Loss VS Epochs')\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcEpnYg_jJz6"
   },
   "source": [
    "<h2>(1) RNN with 1 LSTM layer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xe6W48VkipYp",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           2232096   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,285,397\n",
      "Trainable params: 2,285,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/10\n",
      "56000/56000 [==============================] - ETA: 9:52 - loss: 0.6932 - acc: 0.476 - ETA: 6:01 - loss: 0.6908 - acc: 0.648 - ETA: 4:53 - loss: 0.6883 - acc: 0.703 - ETA: 4:16 - loss: 0.6855 - acc: 0.737 - ETA: 4:00 - loss: 0.6823 - acc: 0.756 - ETA: 3:45 - loss: 0.6785 - acc: 0.772 - ETA: 3:35 - loss: 0.6741 - acc: 0.783 - ETA: 3:23 - loss: 0.6696 - acc: 0.788 - ETA: 3:17 - loss: 0.6639 - acc: 0.793 - ETA: 3:10 - loss: 0.6575 - acc: 0.795 - ETA: 3:04 - loss: 0.6493 - acc: 0.797 - ETA: 2:58 - loss: 0.6367 - acc: 0.802 - ETA: 2:52 - loss: 0.6209 - acc: 0.806 - ETA: 2:48 - loss: 0.6134 - acc: 0.808 - ETA: 2:44 - loss: 0.6027 - acc: 0.811 - ETA: 2:39 - loss: 0.5954 - acc: 0.811 - ETA: 2:36 - loss: 0.5841 - acc: 0.814 - ETA: 2:33 - loss: 0.5744 - acc: 0.817 - ETA: 2:30 - loss: 0.5670 - acc: 0.819 - ETA: 2:26 - loss: 0.5611 - acc: 0.820 - ETA: 2:23 - loss: 0.5566 - acc: 0.820 - ETA: 2:21 - loss: 0.5506 - acc: 0.822 - ETA: 2:19 - loss: 0.5459 - acc: 0.822 - ETA: 2:16 - loss: 0.5406 - acc: 0.824 - ETA: 2:14 - loss: 0.5353 - acc: 0.825 - ETA: 2:11 - loss: 0.5305 - acc: 0.826 - ETA: 2:09 - loss: 0.5252 - acc: 0.827 - ETA: 2:06 - loss: 0.5206 - acc: 0.828 - ETA: 2:04 - loss: 0.5165 - acc: 0.829 - ETA: 2:02 - loss: 0.5158 - acc: 0.828 - ETA: 2:00 - loss: 0.5137 - acc: 0.828 - ETA: 1:58 - loss: 0.5095 - acc: 0.830 - ETA: 1:56 - loss: 0.5071 - acc: 0.830 - ETA: 1:54 - loss: 0.5052 - acc: 0.830 - ETA: 1:52 - loss: 0.5029 - acc: 0.830 - ETA: 1:50 - loss: 0.5004 - acc: 0.830 - ETA: 1:48 - loss: 0.4983 - acc: 0.830 - ETA: 1:46 - loss: 0.4973 - acc: 0.830 - ETA: 1:44 - loss: 0.4948 - acc: 0.830 - ETA: 1:42 - loss: 0.4925 - acc: 0.831 - ETA: 1:40 - loss: 0.4900 - acc: 0.831 - ETA: 1:39 - loss: 0.4877 - acc: 0.832 - ETA: 1:37 - loss: 0.4848 - acc: 0.833 - ETA: 1:35 - loss: 0.4833 - acc: 0.832 - ETA: 1:34 - loss: 0.4812 - acc: 0.833 - ETA: 1:32 - loss: 0.4796 - acc: 0.833 - ETA: 1:30 - loss: 0.4779 - acc: 0.833 - ETA: 1:29 - loss: 0.4763 - acc: 0.833 - ETA: 1:27 - loss: 0.4738 - acc: 0.833 - ETA: 1:25 - loss: 0.4723 - acc: 0.833 - ETA: 1:24 - loss: 0.4715 - acc: 0.833 - ETA: 1:22 - loss: 0.4699 - acc: 0.833 - ETA: 1:20 - loss: 0.4684 - acc: 0.833 - ETA: 1:19 - loss: 0.4663 - acc: 0.833 - ETA: 1:17 - loss: 0.4649 - acc: 0.833 - ETA: 1:16 - loss: 0.4632 - acc: 0.833 - ETA: 1:14 - loss: 0.4620 - acc: 0.833 - ETA: 1:13 - loss: 0.4607 - acc: 0.833 - ETA: 1:11 - loss: 0.4589 - acc: 0.833 - ETA: 1:10 - loss: 0.4574 - acc: 0.833 - ETA: 1:08 - loss: 0.4556 - acc: 0.833 - ETA: 1:07 - loss: 0.4537 - acc: 0.834 - ETA: 1:05 - loss: 0.4518 - acc: 0.834 - ETA: 1:04 - loss: 0.4494 - acc: 0.834 - ETA: 1:02 - loss: 0.4473 - acc: 0.835 - ETA: 1:01 - loss: 0.4461 - acc: 0.834 - ETA: 59s - loss: 0.4453 - acc: 0.834 - ETA: 58s - loss: 0.4438 - acc: 0.83 - ETA: 56s - loss: 0.4419 - acc: 0.83 - ETA: 55s - loss: 0.4399 - acc: 0.83 - ETA: 53s - loss: 0.4383 - acc: 0.83 - ETA: 52s - loss: 0.4364 - acc: 0.83 - ETA: 51s - loss: 0.4342 - acc: 0.83 - ETA: 49s - loss: 0.4320 - acc: 0.83 - ETA: 48s - loss: 0.4298 - acc: 0.83 - ETA: 46s - loss: 0.4282 - acc: 0.83 - ETA: 45s - loss: 0.4275 - acc: 0.83 - ETA: 43s - loss: 0.4253 - acc: 0.83 - ETA: 42s - loss: 0.4234 - acc: 0.83 - ETA: 40s - loss: 0.4215 - acc: 0.83 - ETA: 39s - loss: 0.4198 - acc: 0.84 - ETA: 38s - loss: 0.4182 - acc: 0.84 - ETA: 36s - loss: 0.4166 - acc: 0.84 - ETA: 35s - loss: 0.4147 - acc: 0.84 - ETA: 33s - loss: 0.4125 - acc: 0.84 - ETA: 32s - loss: 0.4109 - acc: 0.84 - ETA: 31s - loss: 0.4092 - acc: 0.84 - ETA: 29s - loss: 0.4078 - acc: 0.84 - ETA: 28s - loss: 0.4062 - acc: 0.84 - ETA: 26s - loss: 0.4048 - acc: 0.84 - ETA: 25s - loss: 0.4032 - acc: 0.84 - ETA: 24s - loss: 0.4013 - acc: 0.84 - ETA: 22s - loss: 0.3997 - acc: 0.84 - ETA: 21s - loss: 0.3979 - acc: 0.84 - ETA: 19s - loss: 0.3969 - acc: 0.84 - ETA: 18s - loss: 0.3952 - acc: 0.84 - ETA: 17s - loss: 0.3938 - acc: 0.84 - ETA: 15s - loss: 0.3923 - acc: 0.84 - ETA: 14s - loss: 0.3907 - acc: 0.85 - ETA: 12s - loss: 0.3892 - acc: 0.85 - ETA: 11s - loss: 0.3873 - acc: 0.85 - ETA: 10s - loss: 0.3857 - acc: 0.85 - ETA: 8s - loss: 0.3842 - acc: 0.8523 - ETA: 7s - loss: 0.3827 - acc: 0.852 - ETA: 6s - loss: 0.3811 - acc: 0.853 - ETA: 4s - loss: 0.3799 - acc: 0.853 - ETA: 3s - loss: 0.3784 - acc: 0.854 - ETA: 1s - loss: 0.3770 - acc: 0.855 - ETA: 0s - loss: 0.3759 - acc: 0.855 - 162s 3ms/step - loss: 0.3756 - acc: 0.8554 - val_loss: 0.2226 - val_acc: 0.9071\n",
      "Epoch 2/10\n",
      "56000/56000 [==============================] - ETA: 2:54 - loss: 0.1961 - acc: 0.918 - ETA: 2:52 - loss: 0.1844 - acc: 0.931 - ETA: 2:48 - loss: 0.2128 - acc: 0.914 - ETA: 2:45 - loss: 0.2012 - acc: 0.919 - ETA: 2:42 - loss: 0.2107 - acc: 0.918 - ETA: 2:38 - loss: 0.2095 - acc: 0.917 - ETA: 2:35 - loss: 0.2111 - acc: 0.918 - ETA: 2:30 - loss: 0.2055 - acc: 0.921 - ETA: 2:27 - loss: 0.2057 - acc: 0.921 - ETA: 2:23 - loss: 0.1999 - acc: 0.923 - ETA: 2:21 - loss: 0.2000 - acc: 0.924 - ETA: 2:19 - loss: 0.1979 - acc: 0.925 - ETA: 2:17 - loss: 0.1962 - acc: 0.926 - ETA: 2:14 - loss: 0.1996 - acc: 0.924 - ETA: 2:12 - loss: 0.1991 - acc: 0.924 - ETA: 2:10 - loss: 0.1993 - acc: 0.923 - ETA: 2:08 - loss: 0.1983 - acc: 0.924 - ETA: 2:07 - loss: 0.2004 - acc: 0.923 - ETA: 2:05 - loss: 0.1998 - acc: 0.923 - ETA: 2:03 - loss: 0.2002 - acc: 0.924 - ETA: 2:01 - loss: 0.1991 - acc: 0.924 - ETA: 2:00 - loss: 0.2004 - acc: 0.923 - ETA: 1:58 - loss: 0.1992 - acc: 0.923 - ETA: 1:57 - loss: 0.1980 - acc: 0.924 - ETA: 1:55 - loss: 0.1973 - acc: 0.925 - ETA: 1:54 - loss: 0.1974 - acc: 0.925 - ETA: 1:52 - loss: 0.1959 - acc: 0.925 - ETA: 1:50 - loss: 0.1962 - acc: 0.925 - ETA: 1:49 - loss: 0.1960 - acc: 0.925 - ETA: 1:48 - loss: 0.1958 - acc: 0.925 - ETA: 1:46 - loss: 0.1968 - acc: 0.925 - ETA: 1:45 - loss: 0.1958 - acc: 0.925 - ETA: 1:43 - loss: 0.1954 - acc: 0.925 - ETA: 1:42 - loss: 0.1956 - acc: 0.925 - ETA: 1:40 - loss: 0.1962 - acc: 0.924 - ETA: 1:39 - loss: 0.1962 - acc: 0.924 - ETA: 1:37 - loss: 0.1955 - acc: 0.925 - ETA: 1:36 - loss: 0.1951 - acc: 0.925 - ETA: 1:34 - loss: 0.1957 - acc: 0.924 - ETA: 1:33 - loss: 0.1959 - acc: 0.924 - ETA: 1:32 - loss: 0.1952 - acc: 0.925 - ETA: 1:30 - loss: 0.1950 - acc: 0.925 - ETA: 1:29 - loss: 0.1945 - acc: 0.925 - ETA: 1:27 - loss: 0.1950 - acc: 0.925 - ETA: 1:26 - loss: 0.1943 - acc: 0.925 - ETA: 1:25 - loss: 0.1943 - acc: 0.925 - ETA: 1:23 - loss: 0.1940 - acc: 0.924 - ETA: 1:22 - loss: 0.1942 - acc: 0.924 - ETA: 1:20 - loss: 0.1943 - acc: 0.924 - ETA: 1:19 - loss: 0.1946 - acc: 0.924 - ETA: 1:18 - loss: 0.1946 - acc: 0.924 - ETA: 1:16 - loss: 0.1937 - acc: 0.924 - ETA: 1:15 - loss: 0.1930 - acc: 0.925 - ETA: 1:13 - loss: 0.1927 - acc: 0.925 - ETA: 1:12 - loss: 0.1920 - acc: 0.925 - ETA: 1:11 - loss: 0.1920 - acc: 0.925 - ETA: 1:09 - loss: 0.1917 - acc: 0.925 - ETA: 1:08 - loss: 0.1917 - acc: 0.925 - ETA: 1:07 - loss: 0.1909 - acc: 0.925 - ETA: 1:05 - loss: 0.1907 - acc: 0.925 - ETA: 1:04 - loss: 0.1908 - acc: 0.925 - ETA: 1:02 - loss: 0.1912 - acc: 0.925 - ETA: 1:01 - loss: 0.1906 - acc: 0.925 - ETA: 1:00 - loss: 0.1902 - acc: 0.925 - ETA: 59s - loss: 0.1902 - acc: 0.925 - ETA: 57s - loss: 0.1901 - acc: 0.92 - ETA: 56s - loss: 0.1900 - acc: 0.92 - ETA: 55s - loss: 0.1898 - acc: 0.92 - ETA: 54s - loss: 0.1893 - acc: 0.92 - ETA: 52s - loss: 0.1892 - acc: 0.92 - ETA: 51s - loss: 0.1892 - acc: 0.92 - ETA: 50s - loss: 0.1893 - acc: 0.92 - ETA: 48s - loss: 0.1891 - acc: 0.92 - ETA: 47s - loss: 0.1886 - acc: 0.92 - ETA: 46s - loss: 0.1880 - acc: 0.92 - ETA: 44s - loss: 0.1878 - acc: 0.92 - ETA: 43s - loss: 0.1875 - acc: 0.92 - ETA: 42s - loss: 0.1880 - acc: 0.92 - ETA: 40s - loss: 0.1882 - acc: 0.92 - ETA: 39s - loss: 0.1882 - acc: 0.92 - ETA: 37s - loss: 0.1883 - acc: 0.92 - ETA: 36s - loss: 0.1884 - acc: 0.92 - ETA: 35s - loss: 0.1883 - acc: 0.92 - ETA: 33s - loss: 0.1881 - acc: 0.92 - ETA: 32s - loss: 0.1877 - acc: 0.92 - ETA: 31s - loss: 0.1876 - acc: 0.92 - ETA: 29s - loss: 0.1873 - acc: 0.92 - ETA: 28s - loss: 0.1872 - acc: 0.92 - ETA: 27s - loss: 0.1873 - acc: 0.92 - ETA: 25s - loss: 0.1874 - acc: 0.92 - ETA: 24s - loss: 0.1877 - acc: 0.92 - ETA: 23s - loss: 0.1876 - acc: 0.92 - ETA: 21s - loss: 0.1877 - acc: 0.92 - ETA: 20s - loss: 0.1877 - acc: 0.92 - ETA: 19s - loss: 0.1877 - acc: 0.92 - ETA: 17s - loss: 0.1881 - acc: 0.92 - ETA: 16s - loss: 0.1879 - acc: 0.92 - ETA: 15s - loss: 0.1880 - acc: 0.92 - ETA: 13s - loss: 0.1882 - acc: 0.92 - ETA: 12s - loss: 0.1882 - acc: 0.92 - ETA: 11s - loss: 0.1881 - acc: 0.92 - ETA: 9s - loss: 0.1882 - acc: 0.9262 - ETA: 8s - loss: 0.1886 - acc: 0.925 - ETA: 7s - loss: 0.1886 - acc: 0.925 - ETA: 5s - loss: 0.1884 - acc: 0.926 - ETA: 4s - loss: 0.1884 - acc: 0.926 - ETA: 3s - loss: 0.1882 - acc: 0.926 - ETA: 1s - loss: 0.1881 - acc: 0.926 - ETA: 0s - loss: 0.1876 - acc: 0.926 - 156s 3ms/step - loss: 0.1876 - acc: 0.9261 - val_loss: 0.2109 - val_acc: 0.9197\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 2:22 - loss: 0.1536 - acc: 0.939 - ETA: 2:16 - loss: 0.1383 - acc: 0.950 - ETA: 2:12 - loss: 0.1340 - acc: 0.953 - ETA: 2:12 - loss: 0.1393 - acc: 0.947 - ETA: 2:12 - loss: 0.1374 - acc: 0.948 - ETA: 2:10 - loss: 0.1356 - acc: 0.950 - ETA: 2:08 - loss: 0.1339 - acc: 0.950 - ETA: 2:07 - loss: 0.1374 - acc: 0.949 - ETA: 2:06 - loss: 0.1387 - acc: 0.948 - ETA: 2:05 - loss: 0.1394 - acc: 0.946 - ETA: 2:04 - loss: 0.1397 - acc: 0.946 - ETA: 2:03 - loss: 0.1403 - acc: 0.946 - ETA: 2:01 - loss: 0.1407 - acc: 0.946 - ETA: 2:00 - loss: 0.1425 - acc: 0.945 - ETA: 2:00 - loss: 0.1409 - acc: 0.945 - ETA: 1:58 - loss: 0.1433 - acc: 0.944 - ETA: 1:57 - loss: 0.1422 - acc: 0.945 - ETA: 1:55 - loss: 0.1427 - acc: 0.945 - ETA: 1:54 - loss: 0.1415 - acc: 0.945 - ETA: 1:52 - loss: 0.1409 - acc: 0.945 - ETA: 1:51 - loss: 0.1418 - acc: 0.945 - ETA: 1:50 - loss: 0.1415 - acc: 0.945 - ETA: 1:49 - loss: 0.1434 - acc: 0.944 - ETA: 1:48 - loss: 0.1429 - acc: 0.944 - ETA: 1:46 - loss: 0.1423 - acc: 0.945 - ETA: 1:45 - loss: 0.1433 - acc: 0.944 - ETA: 1:44 - loss: 0.1430 - acc: 0.945 - ETA: 1:43 - loss: 0.1438 - acc: 0.945 - ETA: 1:42 - loss: 0.1443 - acc: 0.944 - ETA: 1:40 - loss: 0.1451 - acc: 0.945 - ETA: 1:39 - loss: 0.1453 - acc: 0.945 - ETA: 1:38 - loss: 0.1453 - acc: 0.945 - ETA: 1:37 - loss: 0.1462 - acc: 0.944 - ETA: 1:36 - loss: 0.1458 - acc: 0.944 - ETA: 1:34 - loss: 0.1456 - acc: 0.944 - ETA: 1:33 - loss: 0.1465 - acc: 0.944 - ETA: 1:32 - loss: 0.1460 - acc: 0.944 - ETA: 1:31 - loss: 0.1452 - acc: 0.945 - ETA: 1:29 - loss: 0.1444 - acc: 0.945 - ETA: 1:28 - loss: 0.1450 - acc: 0.945 - ETA: 1:27 - loss: 0.1452 - acc: 0.945 - ETA: 1:26 - loss: 0.1461 - acc: 0.944 - ETA: 1:24 - loss: 0.1460 - acc: 0.944 - ETA: 1:23 - loss: 0.1460 - acc: 0.944 - ETA: 1:22 - loss: 0.1465 - acc: 0.944 - ETA: 1:21 - loss: 0.1460 - acc: 0.944 - ETA: 1:19 - loss: 0.1474 - acc: 0.944 - ETA: 1:18 - loss: 0.1466 - acc: 0.944 - ETA: 1:17 - loss: 0.1478 - acc: 0.943 - ETA: 1:16 - loss: 0.1478 - acc: 0.943 - ETA: 1:14 - loss: 0.1485 - acc: 0.943 - ETA: 1:13 - loss: 0.1487 - acc: 0.943 - ETA: 1:12 - loss: 0.1491 - acc: 0.943 - ETA: 1:11 - loss: 0.1490 - acc: 0.943 - ETA: 1:09 - loss: 0.1481 - acc: 0.943 - ETA: 1:08 - loss: 0.1481 - acc: 0.943 - ETA: 1:07 - loss: 0.1478 - acc: 0.943 - ETA: 1:06 - loss: 0.1479 - acc: 0.943 - ETA: 1:04 - loss: 0.1474 - acc: 0.943 - ETA: 1:03 - loss: 0.1467 - acc: 0.944 - ETA: 1:02 - loss: 0.1470 - acc: 0.943 - ETA: 1:00 - loss: 0.1474 - acc: 0.943 - ETA: 59s - loss: 0.1472 - acc: 0.943 - ETA: 58s - loss: 0.1471 - acc: 0.94 - ETA: 57s - loss: 0.1468 - acc: 0.94 - ETA: 55s - loss: 0.1470 - acc: 0.94 - ETA: 54s - loss: 0.1466 - acc: 0.94 - ETA: 53s - loss: 0.1462 - acc: 0.94 - ETA: 51s - loss: 0.1462 - acc: 0.94 - ETA: 50s - loss: 0.1463 - acc: 0.94 - ETA: 49s - loss: 0.1464 - acc: 0.94 - ETA: 48s - loss: 0.1470 - acc: 0.94 - ETA: 46s - loss: 0.1471 - acc: 0.94 - ETA: 45s - loss: 0.1468 - acc: 0.94 - ETA: 44s - loss: 0.1465 - acc: 0.94 - ETA: 42s - loss: 0.1467 - acc: 0.94 - ETA: 41s - loss: 0.1472 - acc: 0.94 - ETA: 40s - loss: 0.1473 - acc: 0.94 - ETA: 39s - loss: 0.1474 - acc: 0.94 - ETA: 37s - loss: 0.1475 - acc: 0.94 - ETA: 36s - loss: 0.1477 - acc: 0.94 - ETA: 35s - loss: 0.1476 - acc: 0.94 - ETA: 34s - loss: 0.1476 - acc: 0.94 - ETA: 32s - loss: 0.1474 - acc: 0.94 - ETA: 31s - loss: 0.1472 - acc: 0.94 - ETA: 30s - loss: 0.1474 - acc: 0.94 - ETA: 28s - loss: 0.1472 - acc: 0.94 - ETA: 27s - loss: 0.1476 - acc: 0.94 - ETA: 26s - loss: 0.1475 - acc: 0.94 - ETA: 24s - loss: 0.1481 - acc: 0.94 - ETA: 23s - loss: 0.1482 - acc: 0.94 - ETA: 22s - loss: 0.1483 - acc: 0.94 - ETA: 21s - loss: 0.1489 - acc: 0.94 - ETA: 19s - loss: 0.1492 - acc: 0.94 - ETA: 18s - loss: 0.1491 - acc: 0.94 - ETA: 17s - loss: 0.1492 - acc: 0.94 - ETA: 15s - loss: 0.1492 - acc: 0.94 - ETA: 14s - loss: 0.1492 - acc: 0.94 - ETA: 13s - loss: 0.1491 - acc: 0.94 - ETA: 12s - loss: 0.1491 - acc: 0.94 - ETA: 10s - loss: 0.1491 - acc: 0.94 - ETA: 9s - loss: 0.1490 - acc: 0.9422 - ETA: 8s - loss: 0.1489 - acc: 0.942 - ETA: 6s - loss: 0.1490 - acc: 0.942 - ETA: 5s - loss: 0.1494 - acc: 0.942 - ETA: 4s - loss: 0.1492 - acc: 0.942 - ETA: 3s - loss: 0.1493 - acc: 0.942 - ETA: 1s - loss: 0.1493 - acc: 0.942 - ETA: 0s - loss: 0.1491 - acc: 0.942 - 151s 3ms/step - loss: 0.1490 - acc: 0.9423 - val_loss: 0.2188 - val_acc: 0.9172\n",
      "Epoch 4/10\n",
      "56000/56000 [==============================] - ETA: 2:20 - loss: 0.1021 - acc: 0.962 - ETA: 2:19 - loss: 0.1137 - acc: 0.962 - ETA: 2:17 - loss: 0.1197 - acc: 0.961 - ETA: 2:15 - loss: 0.1167 - acc: 0.961 - ETA: 2:12 - loss: 0.1204 - acc: 0.958 - ETA: 2:10 - loss: 0.1245 - acc: 0.954 - ETA: 2:09 - loss: 0.1244 - acc: 0.954 - ETA: 2:09 - loss: 0.1212 - acc: 0.956 - ETA: 2:07 - loss: 0.1227 - acc: 0.955 - ETA: 2:05 - loss: 0.1241 - acc: 0.954 - ETA: 2:04 - loss: 0.1264 - acc: 0.953 - ETA: 2:03 - loss: 0.1265 - acc: 0.953 - ETA: 2:02 - loss: 0.1241 - acc: 0.954 - ETA: 2:01 - loss: 0.1231 - acc: 0.954 - ETA: 1:59 - loss: 0.1219 - acc: 0.954 - ETA: 1:57 - loss: 0.1205 - acc: 0.955 - ETA: 1:56 - loss: 0.1197 - acc: 0.955 - ETA: 1:55 - loss: 0.1200 - acc: 0.955 - ETA: 1:53 - loss: 0.1198 - acc: 0.955 - ETA: 1:52 - loss: 0.1197 - acc: 0.956 - ETA: 1:51 - loss: 0.1183 - acc: 0.957 - ETA: 1:50 - loss: 0.1211 - acc: 0.956 - ETA: 1:48 - loss: 0.1210 - acc: 0.956 - ETA: 1:47 - loss: 0.1204 - acc: 0.956 - ETA: 1:46 - loss: 0.1220 - acc: 0.955 - ETA: 1:44 - loss: 0.1216 - acc: 0.956 - ETA: 1:43 - loss: 0.1215 - acc: 0.956 - ETA: 1:42 - loss: 0.1213 - acc: 0.956 - ETA: 1:40 - loss: 0.1218 - acc: 0.955 - ETA: 1:39 - loss: 0.1221 - acc: 0.955 - ETA: 1:38 - loss: 0.1215 - acc: 0.956 - ETA: 1:37 - loss: 0.1214 - acc: 0.956 - ETA: 1:35 - loss: 0.1208 - acc: 0.956 - ETA: 1:34 - loss: 0.1212 - acc: 0.955 - ETA: 1:33 - loss: 0.1215 - acc: 0.955 - ETA: 1:32 - loss: 0.1231 - acc: 0.954 - ETA: 1:31 - loss: 0.1237 - acc: 0.954 - ETA: 1:30 - loss: 0.1238 - acc: 0.954 - ETA: 1:29 - loss: 0.1240 - acc: 0.954 - ETA: 1:28 - loss: 0.1239 - acc: 0.954 - ETA: 1:26 - loss: 0.1238 - acc: 0.954 - ETA: 1:25 - loss: 0.1236 - acc: 0.954 - ETA: 1:24 - loss: 0.1246 - acc: 0.954 - ETA: 1:23 - loss: 0.1241 - acc: 0.954 - ETA: 1:21 - loss: 0.1233 - acc: 0.954 - ETA: 1:20 - loss: 0.1235 - acc: 0.954 - ETA: 1:19 - loss: 0.1239 - acc: 0.954 - ETA: 1:17 - loss: 0.1232 - acc: 0.954 - ETA: 1:16 - loss: 0.1232 - acc: 0.954 - ETA: 1:15 - loss: 0.1231 - acc: 0.954 - ETA: 1:14 - loss: 0.1229 - acc: 0.954 - ETA: 1:12 - loss: 0.1227 - acc: 0.954 - ETA: 1:11 - loss: 0.1226 - acc: 0.954 - ETA: 1:10 - loss: 0.1231 - acc: 0.954 - ETA: 1:08 - loss: 0.1230 - acc: 0.954 - ETA: 1:07 - loss: 0.1235 - acc: 0.954 - ETA: 1:06 - loss: 0.1235 - acc: 0.954 - ETA: 1:04 - loss: 0.1234 - acc: 0.954 - ETA: 1:03 - loss: 0.1232 - acc: 0.954 - ETA: 1:02 - loss: 0.1233 - acc: 0.954 - ETA: 1:01 - loss: 0.1239 - acc: 0.954 - ETA: 59s - loss: 0.1234 - acc: 0.954 - ETA: 58s - loss: 0.1242 - acc: 0.95 - ETA: 57s - loss: 0.1246 - acc: 0.95 - ETA: 56s - loss: 0.1247 - acc: 0.95 - ETA: 54s - loss: 0.1247 - acc: 0.95 - ETA: 53s - loss: 0.1251 - acc: 0.95 - ETA: 52s - loss: 0.1247 - acc: 0.95 - ETA: 51s - loss: 0.1249 - acc: 0.95 - ETA: 49s - loss: 0.1256 - acc: 0.95 - ETA: 48s - loss: 0.1257 - acc: 0.95 - ETA: 47s - loss: 0.1251 - acc: 0.95 - ETA: 45s - loss: 0.1253 - acc: 0.95 - ETA: 44s - loss: 0.1252 - acc: 0.95 - ETA: 43s - loss: 0.1254 - acc: 0.95 - ETA: 42s - loss: 0.1249 - acc: 0.95 - ETA: 40s - loss: 0.1252 - acc: 0.95 - ETA: 39s - loss: 0.1254 - acc: 0.95 - ETA: 38s - loss: 0.1258 - acc: 0.95 - ETA: 37s - loss: 0.1260 - acc: 0.95 - ETA: 35s - loss: 0.1259 - acc: 0.95 - ETA: 34s - loss: 0.1258 - acc: 0.95 - ETA: 33s - loss: 0.1260 - acc: 0.95 - ETA: 31s - loss: 0.1261 - acc: 0.95 - ETA: 30s - loss: 0.1264 - acc: 0.95 - ETA: 29s - loss: 0.1259 - acc: 0.95 - ETA: 28s - loss: 0.1258 - acc: 0.95 - ETA: 26s - loss: 0.1265 - acc: 0.95 - ETA: 25s - loss: 0.1265 - acc: 0.95 - ETA: 24s - loss: 0.1263 - acc: 0.95 - ETA: 23s - loss: 0.1265 - acc: 0.95 - ETA: 21s - loss: 0.1264 - acc: 0.95 - ETA: 20s - loss: 0.1264 - acc: 0.95 - ETA: 19s - loss: 0.1266 - acc: 0.95 - ETA: 18s - loss: 0.1266 - acc: 0.95 - ETA: 16s - loss: 0.1270 - acc: 0.95 - ETA: 15s - loss: 0.1270 - acc: 0.95 - ETA: 14s - loss: 0.1268 - acc: 0.95 - ETA: 13s - loss: 0.1266 - acc: 0.95 - ETA: 11s - loss: 0.1266 - acc: 0.95 - ETA: 10s - loss: 0.1266 - acc: 0.95 - ETA: 9s - loss: 0.1268 - acc: 0.9525 - ETA: 8s - loss: 0.1271 - acc: 0.952 - ETA: 6s - loss: 0.1270 - acc: 0.952 - ETA: 5s - loss: 0.1275 - acc: 0.952 - ETA: 4s - loss: 0.1275 - acc: 0.952 - ETA: 2s - loss: 0.1277 - acc: 0.952 - ETA: 1s - loss: 0.1281 - acc: 0.951 - ETA: 0s - loss: 0.1280 - acc: 0.952 - 148s 3ms/step - loss: 0.1279 - acc: 0.9520 - val_loss: 0.2365 - val_acc: 0.9152\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 2:18 - loss: 0.1124 - acc: 0.960 - ETA: 2:14 - loss: 0.1178 - acc: 0.958 - ETA: 2:10 - loss: 0.1047 - acc: 0.962 - ETA: 2:09 - loss: 0.1023 - acc: 0.963 - ETA: 2:09 - loss: 0.0981 - acc: 0.965 - ETA: 2:09 - loss: 0.0979 - acc: 0.965 - ETA: 2:07 - loss: 0.1027 - acc: 0.963 - ETA: 2:05 - loss: 0.1021 - acc: 0.962 - ETA: 2:04 - loss: 0.1053 - acc: 0.961 - ETA: 2:03 - loss: 0.1071 - acc: 0.960 - ETA: 2:02 - loss: 0.1074 - acc: 0.960 - ETA: 2:01 - loss: 0.1068 - acc: 0.961 - ETA: 2:00 - loss: 0.1071 - acc: 0.961 - ETA: 1:58 - loss: 0.1071 - acc: 0.961 - ETA: 1:57 - loss: 0.1072 - acc: 0.961 - ETA: 1:56 - loss: 0.1091 - acc: 0.960 - ETA: 1:55 - loss: 0.1086 - acc: 0.960 - ETA: 1:54 - loss: 0.1068 - acc: 0.960 - ETA: 1:52 - loss: 0.1053 - acc: 0.961 - ETA: 1:51 - loss: 0.1044 - acc: 0.961 - ETA: 1:51 - loss: 0.1043 - acc: 0.961 - ETA: 1:49 - loss: 0.1048 - acc: 0.961 - ETA: 1:48 - loss: 0.1047 - acc: 0.961 - ETA: 1:47 - loss: 0.1031 - acc: 0.962 - ETA: 1:46 - loss: 0.1029 - acc: 0.962 - ETA: 1:45 - loss: 0.1046 - acc: 0.961 - ETA: 1:43 - loss: 0.1044 - acc: 0.961 - ETA: 1:42 - loss: 0.1046 - acc: 0.961 - ETA: 1:41 - loss: 0.1057 - acc: 0.961 - ETA: 1:40 - loss: 0.1047 - acc: 0.961 - ETA: 1:39 - loss: 0.1051 - acc: 0.961 - ETA: 1:37 - loss: 0.1051 - acc: 0.962 - ETA: 1:36 - loss: 0.1046 - acc: 0.962 - ETA: 1:35 - loss: 0.1045 - acc: 0.962 - ETA: 1:34 - loss: 0.1048 - acc: 0.961 - ETA: 1:32 - loss: 0.1049 - acc: 0.961 - ETA: 1:31 - loss: 0.1046 - acc: 0.961 - ETA: 1:30 - loss: 0.1052 - acc: 0.961 - ETA: 1:29 - loss: 0.1055 - acc: 0.961 - ETA: 1:27 - loss: 0.1052 - acc: 0.961 - ETA: 1:26 - loss: 0.1055 - acc: 0.961 - ETA: 1:25 - loss: 0.1062 - acc: 0.961 - ETA: 1:24 - loss: 0.1053 - acc: 0.961 - ETA: 1:22 - loss: 0.1064 - acc: 0.961 - ETA: 1:21 - loss: 0.1068 - acc: 0.961 - ETA: 1:20 - loss: 0.1064 - acc: 0.961 - ETA: 1:19 - loss: 0.1069 - acc: 0.961 - ETA: 1:17 - loss: 0.1072 - acc: 0.960 - ETA: 1:16 - loss: 0.1082 - acc: 0.960 - ETA: 1:15 - loss: 0.1079 - acc: 0.960 - ETA: 1:13 - loss: 0.1077 - acc: 0.960 - ETA: 1:12 - loss: 0.1076 - acc: 0.960 - ETA: 1:11 - loss: 0.1075 - acc: 0.960 - ETA: 1:10 - loss: 0.1076 - acc: 0.960 - ETA: 1:08 - loss: 0.1081 - acc: 0.960 - ETA: 1:07 - loss: 0.1078 - acc: 0.960 - ETA: 1:06 - loss: 0.1079 - acc: 0.960 - ETA: 1:05 - loss: 0.1077 - acc: 0.960 - ETA: 1:03 - loss: 0.1085 - acc: 0.960 - ETA: 1:02 - loss: 0.1083 - acc: 0.960 - ETA: 1:01 - loss: 0.1082 - acc: 0.960 - ETA: 1:00 - loss: 0.1085 - acc: 0.960 - ETA: 58s - loss: 0.1085 - acc: 0.960 - ETA: 57s - loss: 0.1085 - acc: 0.96 - ETA: 56s - loss: 0.1085 - acc: 0.96 - ETA: 55s - loss: 0.1095 - acc: 0.95 - ETA: 53s - loss: 0.1092 - acc: 0.96 - ETA: 52s - loss: 0.1088 - acc: 0.96 - ETA: 51s - loss: 0.1084 - acc: 0.96 - ETA: 50s - loss: 0.1083 - acc: 0.96 - ETA: 48s - loss: 0.1082 - acc: 0.96 - ETA: 47s - loss: 0.1082 - acc: 0.96 - ETA: 46s - loss: 0.1083 - acc: 0.96 - ETA: 45s - loss: 0.1080 - acc: 0.96 - ETA: 43s - loss: 0.1080 - acc: 0.96 - ETA: 42s - loss: 0.1080 - acc: 0.96 - ETA: 41s - loss: 0.1084 - acc: 0.96 - ETA: 40s - loss: 0.1085 - acc: 0.96 - ETA: 38s - loss: 0.1082 - acc: 0.96 - ETA: 37s - loss: 0.1081 - acc: 0.96 - ETA: 36s - loss: 0.1080 - acc: 0.96 - ETA: 34s - loss: 0.1083 - acc: 0.95 - ETA: 33s - loss: 0.1082 - acc: 0.95 - ETA: 32s - loss: 0.1083 - acc: 0.95 - ETA: 31s - loss: 0.1083 - acc: 0.95 - ETA: 29s - loss: 0.1085 - acc: 0.95 - ETA: 28s - loss: 0.1087 - acc: 0.95 - ETA: 27s - loss: 0.1085 - acc: 0.95 - ETA: 26s - loss: 0.1083 - acc: 0.95 - ETA: 24s - loss: 0.1090 - acc: 0.95 - ETA: 23s - loss: 0.1090 - acc: 0.95 - ETA: 22s - loss: 0.1092 - acc: 0.95 - ETA: 20s - loss: 0.1091 - acc: 0.95 - ETA: 19s - loss: 0.1089 - acc: 0.95 - ETA: 18s - loss: 0.1087 - acc: 0.95 - ETA: 17s - loss: 0.1091 - acc: 0.95 - ETA: 15s - loss: 0.1090 - acc: 0.95 - ETA: 14s - loss: 0.1094 - acc: 0.95 - ETA: 13s - loss: 0.1096 - acc: 0.95 - ETA: 11s - loss: 0.1100 - acc: 0.95 - ETA: 10s - loss: 0.1104 - acc: 0.95 - ETA: 9s - loss: 0.1106 - acc: 0.9591 - ETA: 8s - loss: 0.1106 - acc: 0.959 - ETA: 6s - loss: 0.1105 - acc: 0.959 - ETA: 5s - loss: 0.1108 - acc: 0.958 - ETA: 4s - loss: 0.1107 - acc: 0.958 - ETA: 3s - loss: 0.1106 - acc: 0.958 - ETA: 1s - loss: 0.1106 - acc: 0.958 - ETA: 0s - loss: 0.1104 - acc: 0.958 - 150s 3ms/step - loss: 0.1107 - acc: 0.9588 - val_loss: 0.2481 - val_acc: 0.9134\n",
      "Epoch 6/10\n",
      "56000/56000 [==============================] - ETA: 2:17 - loss: 0.0826 - acc: 0.964 - ETA: 2:14 - loss: 0.0872 - acc: 0.970 - ETA: 2:14 - loss: 0.0875 - acc: 0.972 - ETA: 2:13 - loss: 0.0826 - acc: 0.973 - ETA: 2:11 - loss: 0.0812 - acc: 0.974 - ETA: 2:09 - loss: 0.0889 - acc: 0.971 - ETA: 2:07 - loss: 0.0841 - acc: 0.971 - ETA: 2:06 - loss: 0.0839 - acc: 0.971 - ETA: 2:06 - loss: 0.0868 - acc: 0.971 - ETA: 2:04 - loss: 0.0873 - acc: 0.971 - ETA: 2:02 - loss: 0.0851 - acc: 0.972 - ETA: 2:01 - loss: 0.0849 - acc: 0.972 - ETA: 2:00 - loss: 0.0845 - acc: 0.972 - ETA: 1:59 - loss: 0.0855 - acc: 0.972 - ETA: 1:58 - loss: 0.0841 - acc: 0.972 - ETA: 1:56 - loss: 0.0840 - acc: 0.972 - ETA: 1:55 - loss: 0.0846 - acc: 0.972 - ETA: 1:54 - loss: 0.0842 - acc: 0.972 - ETA: 1:53 - loss: 0.0849 - acc: 0.971 - ETA: 1:51 - loss: 0.0860 - acc: 0.971 - ETA: 1:50 - loss: 0.0863 - acc: 0.971 - ETA: 1:49 - loss: 0.0863 - acc: 0.971 - ETA: 1:48 - loss: 0.0863 - acc: 0.971 - ETA: 1:47 - loss: 0.0865 - acc: 0.971 - ETA: 1:46 - loss: 0.0873 - acc: 0.971 - ETA: 1:45 - loss: 0.0875 - acc: 0.970 - ETA: 1:44 - loss: 0.0869 - acc: 0.971 - ETA: 1:43 - loss: 0.0864 - acc: 0.971 - ETA: 1:41 - loss: 0.0867 - acc: 0.970 - ETA: 1:40 - loss: 0.0858 - acc: 0.971 - ETA: 1:39 - loss: 0.0859 - acc: 0.971 - ETA: 1:38 - loss: 0.0870 - acc: 0.970 - ETA: 1:37 - loss: 0.0873 - acc: 0.970 - ETA: 1:36 - loss: 0.0886 - acc: 0.969 - ETA: 1:35 - loss: 0.0892 - acc: 0.969 - ETA: 1:33 - loss: 0.0890 - acc: 0.969 - ETA: 1:32 - loss: 0.0886 - acc: 0.969 - ETA: 1:31 - loss: 0.0883 - acc: 0.969 - ETA: 1:30 - loss: 0.0889 - acc: 0.969 - ETA: 1:28 - loss: 0.0891 - acc: 0.969 - ETA: 1:27 - loss: 0.0888 - acc: 0.969 - ETA: 1:26 - loss: 0.0895 - acc: 0.969 - ETA: 1:25 - loss: 0.0893 - acc: 0.969 - ETA: 1:23 - loss: 0.0892 - acc: 0.969 - ETA: 1:22 - loss: 0.0894 - acc: 0.969 - ETA: 1:21 - loss: 0.0901 - acc: 0.968 - ETA: 1:19 - loss: 0.0898 - acc: 0.968 - ETA: 1:18 - loss: 0.0910 - acc: 0.968 - ETA: 1:17 - loss: 0.0907 - acc: 0.968 - ETA: 1:16 - loss: 0.0904 - acc: 0.968 - ETA: 1:14 - loss: 0.0905 - acc: 0.968 - ETA: 1:13 - loss: 0.0903 - acc: 0.968 - ETA: 1:12 - loss: 0.0904 - acc: 0.968 - ETA: 1:10 - loss: 0.0910 - acc: 0.968 - ETA: 1:09 - loss: 0.0912 - acc: 0.968 - ETA: 1:08 - loss: 0.0917 - acc: 0.968 - ETA: 1:07 - loss: 0.0916 - acc: 0.968 - ETA: 1:05 - loss: 0.0916 - acc: 0.967 - ETA: 1:04 - loss: 0.0924 - acc: 0.967 - ETA: 1:03 - loss: 0.0924 - acc: 0.967 - ETA: 1:01 - loss: 0.0930 - acc: 0.967 - ETA: 1:00 - loss: 0.0931 - acc: 0.967 - ETA: 59s - loss: 0.0935 - acc: 0.967 - ETA: 57s - loss: 0.0938 - acc: 0.96 - ETA: 56s - loss: 0.0937 - acc: 0.96 - ETA: 55s - loss: 0.0937 - acc: 0.96 - ETA: 54s - loss: 0.0938 - acc: 0.96 - ETA: 52s - loss: 0.0940 - acc: 0.96 - ETA: 51s - loss: 0.0941 - acc: 0.96 - ETA: 50s - loss: 0.0944 - acc: 0.96 - ETA: 48s - loss: 0.0947 - acc: 0.96 - ETA: 47s - loss: 0.0948 - acc: 0.96 - ETA: 46s - loss: 0.0950 - acc: 0.96 - ETA: 45s - loss: 0.0953 - acc: 0.96 - ETA: 43s - loss: 0.0957 - acc: 0.96 - ETA: 42s - loss: 0.0960 - acc: 0.96 - ETA: 41s - loss: 0.0960 - acc: 0.96 - ETA: 40s - loss: 0.0958 - acc: 0.96 - ETA: 38s - loss: 0.0960 - acc: 0.96 - ETA: 37s - loss: 0.0963 - acc: 0.96 - ETA: 36s - loss: 0.0961 - acc: 0.96 - ETA: 34s - loss: 0.0960 - acc: 0.96 - ETA: 33s - loss: 0.0956 - acc: 0.96 - ETA: 32s - loss: 0.0959 - acc: 0.96 - ETA: 31s - loss: 0.0962 - acc: 0.96 - ETA: 29s - loss: 0.0964 - acc: 0.96 - ETA: 28s - loss: 0.0964 - acc: 0.96 - ETA: 27s - loss: 0.0960 - acc: 0.96 - ETA: 25s - loss: 0.0963 - acc: 0.96 - ETA: 24s - loss: 0.0963 - acc: 0.96 - ETA: 23s - loss: 0.0963 - acc: 0.96 - ETA: 22s - loss: 0.0964 - acc: 0.96 - ETA: 20s - loss: 0.0965 - acc: 0.96 - ETA: 19s - loss: 0.0969 - acc: 0.96 - ETA: 18s - loss: 0.0971 - acc: 0.96 - ETA: 17s - loss: 0.0973 - acc: 0.96 - ETA: 15s - loss: 0.0972 - acc: 0.96 - ETA: 14s - loss: 0.0975 - acc: 0.96 - ETA: 13s - loss: 0.0978 - acc: 0.96 - ETA: 11s - loss: 0.0982 - acc: 0.96 - ETA: 10s - loss: 0.0983 - acc: 0.96 - ETA: 9s - loss: 0.0985 - acc: 0.9646 - ETA: 8s - loss: 0.0987 - acc: 0.964 - ETA: 6s - loss: 0.0988 - acc: 0.964 - ETA: 5s - loss: 0.0990 - acc: 0.964 - ETA: 4s - loss: 0.0992 - acc: 0.964 - ETA: 3s - loss: 0.0993 - acc: 0.964 - ETA: 1s - loss: 0.0993 - acc: 0.964 - ETA: 0s - loss: 0.0993 - acc: 0.964 - 149s 3ms/step - loss: 0.0992 - acc: 0.9641 - val_loss: 0.2948 - val_acc: 0.9109\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 2:25 - loss: 0.0881 - acc: 0.970 - ETA: 2:22 - loss: 0.0950 - acc: 0.963 - ETA: 2:17 - loss: 0.0995 - acc: 0.966 - ETA: 2:13 - loss: 0.0949 - acc: 0.968 - ETA: 2:11 - loss: 0.0927 - acc: 0.968 - ETA: 2:10 - loss: 0.0891 - acc: 0.969 - ETA: 2:10 - loss: 0.0895 - acc: 0.969 - ETA: 2:08 - loss: 0.0872 - acc: 0.970 - ETA: 2:06 - loss: 0.0910 - acc: 0.969 - ETA: 2:04 - loss: 0.0897 - acc: 0.970 - ETA: 2:03 - loss: 0.0906 - acc: 0.970 - ETA: 2:02 - loss: 0.0895 - acc: 0.970 - ETA: 2:01 - loss: 0.0887 - acc: 0.970 - ETA: 1:59 - loss: 0.0873 - acc: 0.971 - ETA: 1:58 - loss: 0.0865 - acc: 0.971 - ETA: 1:57 - loss: 0.0884 - acc: 0.970 - ETA: 1:56 - loss: 0.0901 - acc: 0.969 - ETA: 1:54 - loss: 0.0891 - acc: 0.970 - ETA: 1:53 - loss: 0.0889 - acc: 0.970 - ETA: 1:51 - loss: 0.0897 - acc: 0.969 - ETA: 1:50 - loss: 0.0889 - acc: 0.970 - ETA: 1:49 - loss: 0.0893 - acc: 0.970 - ETA: 1:47 - loss: 0.0888 - acc: 0.970 - ETA: 1:46 - loss: 0.0884 - acc: 0.970 - ETA: 1:45 - loss: 0.0897 - acc: 0.970 - ETA: 1:44 - loss: 0.0887 - acc: 0.970 - ETA: 1:42 - loss: 0.0881 - acc: 0.971 - ETA: 1:41 - loss: 0.0878 - acc: 0.971 - ETA: 1:40 - loss: 0.0867 - acc: 0.971 - ETA: 1:39 - loss: 0.0872 - acc: 0.971 - ETA: 1:37 - loss: 0.0865 - acc: 0.971 - ETA: 1:36 - loss: 0.0857 - acc: 0.971 - ETA: 1:35 - loss: 0.0856 - acc: 0.971 - ETA: 1:33 - loss: 0.0853 - acc: 0.971 - ETA: 1:32 - loss: 0.0861 - acc: 0.971 - ETA: 1:31 - loss: 0.0873 - acc: 0.970 - ETA: 1:30 - loss: 0.0873 - acc: 0.970 - ETA: 1:29 - loss: 0.0876 - acc: 0.970 - ETA: 1:27 - loss: 0.0874 - acc: 0.970 - ETA: 1:26 - loss: 0.0876 - acc: 0.970 - ETA: 1:25 - loss: 0.0870 - acc: 0.970 - ETA: 1:24 - loss: 0.0867 - acc: 0.970 - ETA: 1:23 - loss: 0.0868 - acc: 0.970 - ETA: 1:21 - loss: 0.0868 - acc: 0.971 - ETA: 1:20 - loss: 0.0870 - acc: 0.970 - ETA: 1:19 - loss: 0.0872 - acc: 0.970 - ETA: 1:18 - loss: 0.0873 - acc: 0.970 - ETA: 1:16 - loss: 0.0877 - acc: 0.970 - ETA: 1:15 - loss: 0.0882 - acc: 0.970 - ETA: 1:14 - loss: 0.0883 - acc: 0.970 - ETA: 1:13 - loss: 0.0886 - acc: 0.970 - ETA: 1:11 - loss: 0.0888 - acc: 0.970 - ETA: 1:10 - loss: 0.0889 - acc: 0.969 - ETA: 1:09 - loss: 0.0894 - acc: 0.969 - ETA: 1:08 - loss: 0.0894 - acc: 0.969 - ETA: 1:06 - loss: 0.0888 - acc: 0.969 - ETA: 1:05 - loss: 0.0891 - acc: 0.969 - ETA: 1:04 - loss: 0.0889 - acc: 0.969 - ETA: 1:03 - loss: 0.0890 - acc: 0.969 - ETA: 1:01 - loss: 0.0890 - acc: 0.969 - ETA: 1:00 - loss: 0.0887 - acc: 0.969 - ETA: 59s - loss: 0.0887 - acc: 0.969 - ETA: 58s - loss: 0.0884 - acc: 0.96 - ETA: 56s - loss: 0.0886 - acc: 0.96 - ETA: 55s - loss: 0.0883 - acc: 0.96 - ETA: 54s - loss: 0.0882 - acc: 0.96 - ETA: 53s - loss: 0.0881 - acc: 0.96 - ETA: 51s - loss: 0.0882 - acc: 0.96 - ETA: 50s - loss: 0.0884 - acc: 0.96 - ETA: 49s - loss: 0.0888 - acc: 0.96 - ETA: 48s - loss: 0.0885 - acc: 0.96 - ETA: 46s - loss: 0.0885 - acc: 0.96 - ETA: 45s - loss: 0.0884 - acc: 0.96 - ETA: 44s - loss: 0.0884 - acc: 0.96 - ETA: 43s - loss: 0.0888 - acc: 0.96 - ETA: 41s - loss: 0.0889 - acc: 0.96 - ETA: 40s - loss: 0.0889 - acc: 0.96 - ETA: 39s - loss: 0.0889 - acc: 0.96 - ETA: 38s - loss: 0.0887 - acc: 0.96 - ETA: 36s - loss: 0.0891 - acc: 0.96 - ETA: 35s - loss: 0.0889 - acc: 0.96 - ETA: 34s - loss: 0.0890 - acc: 0.96 - ETA: 33s - loss: 0.0892 - acc: 0.96 - ETA: 31s - loss: 0.0893 - acc: 0.96 - ETA: 30s - loss: 0.0895 - acc: 0.96 - ETA: 29s - loss: 0.0895 - acc: 0.96 - ETA: 28s - loss: 0.0897 - acc: 0.96 - ETA: 26s - loss: 0.0897 - acc: 0.96 - ETA: 25s - loss: 0.0897 - acc: 0.96 - ETA: 24s - loss: 0.0896 - acc: 0.96 - ETA: 23s - loss: 0.0896 - acc: 0.96 - ETA: 21s - loss: 0.0898 - acc: 0.96 - ETA: 20s - loss: 0.0897 - acc: 0.96 - ETA: 19s - loss: 0.0902 - acc: 0.96 - ETA: 18s - loss: 0.0902 - acc: 0.96 - ETA: 16s - loss: 0.0905 - acc: 0.96 - ETA: 15s - loss: 0.0906 - acc: 0.96 - ETA: 14s - loss: 0.0906 - acc: 0.96 - ETA: 13s - loss: 0.0911 - acc: 0.96 - ETA: 11s - loss: 0.0912 - acc: 0.96 - ETA: 10s - loss: 0.0908 - acc: 0.96 - ETA: 9s - loss: 0.0904 - acc: 0.9680 - ETA: 7s - loss: 0.0904 - acc: 0.968 - ETA: 6s - loss: 0.0906 - acc: 0.967 - ETA: 5s - loss: 0.0905 - acc: 0.967 - ETA: 4s - loss: 0.0904 - acc: 0.967 - ETA: 2s - loss: 0.0906 - acc: 0.967 - ETA: 1s - loss: 0.0903 - acc: 0.968 - ETA: 0s - loss: 0.0905 - acc: 0.967 - 147s 3ms/step - loss: 0.0906 - acc: 0.9679 - val_loss: 0.2935 - val_acc: 0.9061\n",
      "Epoch 8/10\n",
      "56000/56000 [==============================] - ETA: 2:22 - loss: 0.0760 - acc: 0.968 - ETA: 2:15 - loss: 0.0765 - acc: 0.970 - ETA: 2:13 - loss: 0.0737 - acc: 0.974 - ETA: 2:13 - loss: 0.0775 - acc: 0.975 - ETA: 2:12 - loss: 0.0744 - acc: 0.976 - ETA: 2:10 - loss: 0.0731 - acc: 0.976 - ETA: 2:08 - loss: 0.0694 - acc: 0.977 - ETA: 2:06 - loss: 0.0702 - acc: 0.977 - ETA: 2:05 - loss: 0.0710 - acc: 0.976 - ETA: 2:05 - loss: 0.0694 - acc: 0.977 - ETA: 2:03 - loss: 0.0718 - acc: 0.976 - ETA: 2:02 - loss: 0.0706 - acc: 0.977 - ETA: 2:00 - loss: 0.0714 - acc: 0.978 - ETA: 1:59 - loss: 0.0728 - acc: 0.977 - ETA: 1:58 - loss: 0.0725 - acc: 0.977 - ETA: 1:57 - loss: 0.0754 - acc: 0.977 - ETA: 1:55 - loss: 0.0742 - acc: 0.977 - ETA: 1:54 - loss: 0.0740 - acc: 0.977 - ETA: 1:53 - loss: 0.0742 - acc: 0.976 - ETA: 1:52 - loss: 0.0742 - acc: 0.976 - ETA: 1:51 - loss: 0.0755 - acc: 0.975 - ETA: 1:49 - loss: 0.0753 - acc: 0.975 - ETA: 1:48 - loss: 0.0754 - acc: 0.975 - ETA: 1:47 - loss: 0.0759 - acc: 0.975 - ETA: 1:46 - loss: 0.0761 - acc: 0.975 - ETA: 1:45 - loss: 0.0763 - acc: 0.975 - ETA: 1:44 - loss: 0.0753 - acc: 0.975 - ETA: 1:43 - loss: 0.0764 - acc: 0.974 - ETA: 1:41 - loss: 0.0766 - acc: 0.974 - ETA: 1:40 - loss: 0.0774 - acc: 0.974 - ETA: 1:39 - loss: 0.0767 - acc: 0.974 - ETA: 1:38 - loss: 0.0765 - acc: 0.974 - ETA: 1:37 - loss: 0.0763 - acc: 0.974 - ETA: 1:36 - loss: 0.0753 - acc: 0.975 - ETA: 1:34 - loss: 0.0750 - acc: 0.975 - ETA: 1:33 - loss: 0.0749 - acc: 0.975 - ETA: 1:31 - loss: 0.0751 - acc: 0.975 - ETA: 1:30 - loss: 0.0760 - acc: 0.974 - ETA: 1:29 - loss: 0.0757 - acc: 0.975 - ETA: 1:28 - loss: 0.0754 - acc: 0.975 - ETA: 1:26 - loss: 0.0761 - acc: 0.974 - ETA: 1:25 - loss: 0.0759 - acc: 0.975 - ETA: 1:24 - loss: 0.0760 - acc: 0.974 - ETA: 1:22 - loss: 0.0756 - acc: 0.975 - ETA: 1:21 - loss: 0.0750 - acc: 0.975 - ETA: 1:20 - loss: 0.0750 - acc: 0.975 - ETA: 1:18 - loss: 0.0751 - acc: 0.975 - ETA: 1:17 - loss: 0.0752 - acc: 0.975 - ETA: 1:16 - loss: 0.0749 - acc: 0.975 - ETA: 1:15 - loss: 0.0757 - acc: 0.974 - ETA: 1:14 - loss: 0.0760 - acc: 0.974 - ETA: 1:12 - loss: 0.0757 - acc: 0.974 - ETA: 1:11 - loss: 0.0754 - acc: 0.974 - ETA: 1:10 - loss: 0.0760 - acc: 0.974 - ETA: 1:09 - loss: 0.0761 - acc: 0.974 - ETA: 1:07 - loss: 0.0769 - acc: 0.974 - ETA: 1:06 - loss: 0.0769 - acc: 0.973 - ETA: 1:05 - loss: 0.0769 - acc: 0.974 - ETA: 1:04 - loss: 0.0770 - acc: 0.973 - ETA: 1:02 - loss: 0.0773 - acc: 0.973 - ETA: 1:01 - loss: 0.0776 - acc: 0.973 - ETA: 1:00 - loss: 0.0783 - acc: 0.973 - ETA: 59s - loss: 0.0779 - acc: 0.973 - ETA: 57s - loss: 0.0779 - acc: 0.97 - ETA: 56s - loss: 0.0785 - acc: 0.97 - ETA: 55s - loss: 0.0786 - acc: 0.97 - ETA: 53s - loss: 0.0788 - acc: 0.97 - ETA: 52s - loss: 0.0792 - acc: 0.97 - ETA: 51s - loss: 0.0791 - acc: 0.97 - ETA: 50s - loss: 0.0790 - acc: 0.97 - ETA: 48s - loss: 0.0791 - acc: 0.97 - ETA: 47s - loss: 0.0789 - acc: 0.97 - ETA: 46s - loss: 0.0787 - acc: 0.97 - ETA: 45s - loss: 0.0788 - acc: 0.97 - ETA: 43s - loss: 0.0795 - acc: 0.97 - ETA: 42s - loss: 0.0798 - acc: 0.97 - ETA: 41s - loss: 0.0797 - acc: 0.97 - ETA: 40s - loss: 0.0797 - acc: 0.97 - ETA: 38s - loss: 0.0797 - acc: 0.97 - ETA: 37s - loss: 0.0799 - acc: 0.97 - ETA: 36s - loss: 0.0800 - acc: 0.97 - ETA: 34s - loss: 0.0803 - acc: 0.97 - ETA: 33s - loss: 0.0806 - acc: 0.97 - ETA: 32s - loss: 0.0805 - acc: 0.97 - ETA: 31s - loss: 0.0803 - acc: 0.97 - ETA: 29s - loss: 0.0805 - acc: 0.97 - ETA: 28s - loss: 0.0805 - acc: 0.97 - ETA: 27s - loss: 0.0803 - acc: 0.97 - ETA: 26s - loss: 0.0800 - acc: 0.97 - ETA: 24s - loss: 0.0801 - acc: 0.97 - ETA: 23s - loss: 0.0801 - acc: 0.97 - ETA: 22s - loss: 0.0801 - acc: 0.97 - ETA: 20s - loss: 0.0804 - acc: 0.97 - ETA: 19s - loss: 0.0803 - acc: 0.97 - ETA: 18s - loss: 0.0804 - acc: 0.97 - ETA: 17s - loss: 0.0805 - acc: 0.97 - ETA: 15s - loss: 0.0807 - acc: 0.97 - ETA: 14s - loss: 0.0805 - acc: 0.97 - ETA: 13s - loss: 0.0805 - acc: 0.97 - ETA: 11s - loss: 0.0806 - acc: 0.97 - ETA: 10s - loss: 0.0806 - acc: 0.97 - ETA: 9s - loss: 0.0805 - acc: 0.9720 - ETA: 8s - loss: 0.0805 - acc: 0.972 - ETA: 6s - loss: 0.0803 - acc: 0.972 - ETA: 5s - loss: 0.0799 - acc: 0.972 - ETA: 4s - loss: 0.0803 - acc: 0.971 - ETA: 3s - loss: 0.0802 - acc: 0.972 - ETA: 1s - loss: 0.0802 - acc: 0.971 - ETA: 0s - loss: 0.0803 - acc: 0.971 - 150s 3ms/step - loss: 0.0804 - acc: 0.9719 - val_loss: 0.3358 - val_acc: 0.9029\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 2:22 - loss: 0.0551 - acc: 0.982 - ETA: 2:19 - loss: 0.0579 - acc: 0.982 - ETA: 2:18 - loss: 0.0535 - acc: 0.981 - ETA: 2:15 - loss: 0.0488 - acc: 0.983 - ETA: 2:13 - loss: 0.0512 - acc: 0.982 - ETA: 2:11 - loss: 0.0551 - acc: 0.981 - ETA: 2:10 - loss: 0.0548 - acc: 0.982 - ETA: 2:10 - loss: 0.0553 - acc: 0.981 - ETA: 2:08 - loss: 0.0561 - acc: 0.981 - ETA: 2:07 - loss: 0.0569 - acc: 0.981 - ETA: 2:06 - loss: 0.0566 - acc: 0.982 - ETA: 2:05 - loss: 0.0587 - acc: 0.981 - ETA: 2:04 - loss: 0.0593 - acc: 0.981 - ETA: 2:02 - loss: 0.0605 - acc: 0.980 - ETA: 2:01 - loss: 0.0599 - acc: 0.980 - ETA: 1:59 - loss: 0.0621 - acc: 0.979 - ETA: 1:58 - loss: 0.0642 - acc: 0.979 - ETA: 1:57 - loss: 0.0638 - acc: 0.979 - ETA: 1:56 - loss: 0.0640 - acc: 0.979 - ETA: 1:54 - loss: 0.0632 - acc: 0.979 - ETA: 1:53 - loss: 0.0637 - acc: 0.979 - ETA: 1:52 - loss: 0.0631 - acc: 0.980 - ETA: 1:51 - loss: 0.0636 - acc: 0.979 - ETA: 1:49 - loss: 0.0647 - acc: 0.978 - ETA: 1:48 - loss: 0.0649 - acc: 0.978 - ETA: 1:47 - loss: 0.0658 - acc: 0.978 - ETA: 1:46 - loss: 0.0654 - acc: 0.978 - ETA: 1:44 - loss: 0.0651 - acc: 0.978 - ETA: 1:43 - loss: 0.0657 - acc: 0.978 - ETA: 1:41 - loss: 0.0653 - acc: 0.978 - ETA: 1:40 - loss: 0.0659 - acc: 0.978 - ETA: 1:39 - loss: 0.0655 - acc: 0.978 - ETA: 1:37 - loss: 0.0650 - acc: 0.978 - ETA: 1:36 - loss: 0.0651 - acc: 0.977 - ETA: 1:35 - loss: 0.0653 - acc: 0.978 - ETA: 1:33 - loss: 0.0653 - acc: 0.978 - ETA: 1:32 - loss: 0.0645 - acc: 0.978 - ETA: 1:31 - loss: 0.0647 - acc: 0.978 - ETA: 1:29 - loss: 0.0663 - acc: 0.977 - ETA: 1:28 - loss: 0.0665 - acc: 0.977 - ETA: 1:27 - loss: 0.0664 - acc: 0.977 - ETA: 1:25 - loss: 0.0669 - acc: 0.977 - ETA: 1:24 - loss: 0.0676 - acc: 0.977 - ETA: 1:23 - loss: 0.0675 - acc: 0.977 - ETA: 1:21 - loss: 0.0687 - acc: 0.976 - ETA: 1:20 - loss: 0.0693 - acc: 0.976 - ETA: 1:19 - loss: 0.0697 - acc: 0.976 - ETA: 1:18 - loss: 0.0699 - acc: 0.976 - ETA: 1:16 - loss: 0.0699 - acc: 0.976 - ETA: 1:15 - loss: 0.0696 - acc: 0.976 - ETA: 1:14 - loss: 0.0697 - acc: 0.976 - ETA: 1:12 - loss: 0.0696 - acc: 0.976 - ETA: 1:11 - loss: 0.0695 - acc: 0.976 - ETA: 1:10 - loss: 0.0697 - acc: 0.976 - ETA: 1:08 - loss: 0.0700 - acc: 0.976 - ETA: 1:07 - loss: 0.0699 - acc: 0.976 - ETA: 1:06 - loss: 0.0698 - acc: 0.976 - ETA: 1:05 - loss: 0.0698 - acc: 0.976 - ETA: 1:03 - loss: 0.0700 - acc: 0.976 - ETA: 1:02 - loss: 0.0699 - acc: 0.976 - ETA: 1:01 - loss: 0.0701 - acc: 0.976 - ETA: 59s - loss: 0.0702 - acc: 0.976 - ETA: 58s - loss: 0.0702 - acc: 0.97 - ETA: 57s - loss: 0.0703 - acc: 0.97 - ETA: 56s - loss: 0.0702 - acc: 0.97 - ETA: 54s - loss: 0.0704 - acc: 0.97 - ETA: 53s - loss: 0.0707 - acc: 0.97 - ETA: 52s - loss: 0.0707 - acc: 0.97 - ETA: 51s - loss: 0.0711 - acc: 0.97 - ETA: 49s - loss: 0.0717 - acc: 0.97 - ETA: 48s - loss: 0.0720 - acc: 0.97 - ETA: 47s - loss: 0.0721 - acc: 0.97 - ETA: 46s - loss: 0.0726 - acc: 0.97 - ETA: 44s - loss: 0.0727 - acc: 0.97 - ETA: 43s - loss: 0.0724 - acc: 0.97 - ETA: 42s - loss: 0.0728 - acc: 0.97 - ETA: 41s - loss: 0.0729 - acc: 0.97 - ETA: 39s - loss: 0.0730 - acc: 0.97 - ETA: 38s - loss: 0.0732 - acc: 0.97 - ETA: 37s - loss: 0.0729 - acc: 0.97 - ETA: 36s - loss: 0.0730 - acc: 0.97 - ETA: 34s - loss: 0.0734 - acc: 0.97 - ETA: 33s - loss: 0.0734 - acc: 0.97 - ETA: 32s - loss: 0.0732 - acc: 0.97 - ETA: 30s - loss: 0.0731 - acc: 0.97 - ETA: 29s - loss: 0.0731 - acc: 0.97 - ETA: 28s - loss: 0.0730 - acc: 0.97 - ETA: 27s - loss: 0.0734 - acc: 0.97 - ETA: 25s - loss: 0.0730 - acc: 0.97 - ETA: 24s - loss: 0.0729 - acc: 0.97 - ETA: 23s - loss: 0.0731 - acc: 0.97 - ETA: 22s - loss: 0.0734 - acc: 0.97 - ETA: 20s - loss: 0.0734 - acc: 0.97 - ETA: 19s - loss: 0.0734 - acc: 0.97 - ETA: 18s - loss: 0.0734 - acc: 0.97 - ETA: 16s - loss: 0.0735 - acc: 0.97 - ETA: 15s - loss: 0.0737 - acc: 0.97 - ETA: 14s - loss: 0.0736 - acc: 0.97 - ETA: 13s - loss: 0.0736 - acc: 0.97 - ETA: 11s - loss: 0.0736 - acc: 0.97 - ETA: 10s - loss: 0.0740 - acc: 0.97 - ETA: 9s - loss: 0.0743 - acc: 0.9741 - ETA: 8s - loss: 0.0744 - acc: 0.974 - ETA: 6s - loss: 0.0745 - acc: 0.974 - ETA: 5s - loss: 0.0747 - acc: 0.973 - ETA: 4s - loss: 0.0748 - acc: 0.973 - ETA: 3s - loss: 0.0747 - acc: 0.973 - ETA: 1s - loss: 0.0750 - acc: 0.973 - ETA: 0s - loss: 0.0750 - acc: 0.973 - 151s 3ms/step - loss: 0.0754 - acc: 0.9736 - val_loss: 0.3127 - val_acc: 0.9058\n",
      "Epoch 10/10\n",
      "56000/56000 [==============================] - ETA: 2:20 - loss: 0.0595 - acc: 0.984 - ETA: 2:15 - loss: 0.0587 - acc: 0.980 - ETA: 2:13 - loss: 0.0559 - acc: 0.981 - ETA: 2:10 - loss: 0.0573 - acc: 0.980 - ETA: 2:10 - loss: 0.0632 - acc: 0.978 - ETA: 2:10 - loss: 0.0649 - acc: 0.977 - ETA: 2:08 - loss: 0.0650 - acc: 0.978 - ETA: 2:06 - loss: 0.0651 - acc: 0.978 - ETA: 2:05 - loss: 0.0648 - acc: 0.978 - ETA: 2:04 - loss: 0.0648 - acc: 0.978 - ETA: 2:03 - loss: 0.0651 - acc: 0.978 - ETA: 2:02 - loss: 0.0641 - acc: 0.979 - ETA: 2:01 - loss: 0.0641 - acc: 0.979 - ETA: 2:00 - loss: 0.0647 - acc: 0.979 - ETA: 2:00 - loss: 0.0661 - acc: 0.978 - ETA: 2:01 - loss: 0.0659 - acc: 0.978 - ETA: 2:00 - loss: 0.0649 - acc: 0.978 - ETA: 1:59 - loss: 0.0662 - acc: 0.978 - ETA: 1:58 - loss: 0.0664 - acc: 0.978 - ETA: 1:58 - loss: 0.0679 - acc: 0.978 - ETA: 1:56 - loss: 0.0688 - acc: 0.977 - ETA: 1:55 - loss: 0.0684 - acc: 0.978 - ETA: 1:54 - loss: 0.0680 - acc: 0.977 - ETA: 1:52 - loss: 0.0671 - acc: 0.978 - ETA: 1:51 - loss: 0.0669 - acc: 0.977 - ETA: 1:50 - loss: 0.0671 - acc: 0.978 - ETA: 1:48 - loss: 0.0663 - acc: 0.978 - ETA: 1:47 - loss: 0.0663 - acc: 0.978 - ETA: 1:45 - loss: 0.0655 - acc: 0.978 - ETA: 1:44 - loss: 0.0650 - acc: 0.978 - ETA: 1:42 - loss: 0.0645 - acc: 0.979 - ETA: 1:41 - loss: 0.0637 - acc: 0.979 - ETA: 1:39 - loss: 0.0633 - acc: 0.979 - ETA: 1:38 - loss: 0.0648 - acc: 0.979 - ETA: 1:37 - loss: 0.0642 - acc: 0.979 - ETA: 1:35 - loss: 0.0651 - acc: 0.979 - ETA: 1:34 - loss: 0.0653 - acc: 0.979 - ETA: 1:32 - loss: 0.0652 - acc: 0.979 - ETA: 1:31 - loss: 0.0653 - acc: 0.978 - ETA: 1:30 - loss: 0.0648 - acc: 0.978 - ETA: 1:28 - loss: 0.0650 - acc: 0.978 - ETA: 1:27 - loss: 0.0652 - acc: 0.978 - ETA: 1:25 - loss: 0.0656 - acc: 0.978 - ETA: 1:24 - loss: 0.0663 - acc: 0.978 - ETA: 1:23 - loss: 0.0666 - acc: 0.978 - ETA: 1:21 - loss: 0.0666 - acc: 0.978 - ETA: 1:20 - loss: 0.0664 - acc: 0.978 - ETA: 1:19 - loss: 0.0661 - acc: 0.978 - ETA: 1:17 - loss: 0.0656 - acc: 0.978 - ETA: 1:16 - loss: 0.0655 - acc: 0.978 - ETA: 1:15 - loss: 0.0651 - acc: 0.978 - ETA: 1:13 - loss: 0.0652 - acc: 0.978 - ETA: 1:12 - loss: 0.0651 - acc: 0.978 - ETA: 1:11 - loss: 0.0649 - acc: 0.978 - ETA: 1:09 - loss: 0.0648 - acc: 0.978 - ETA: 1:08 - loss: 0.0644 - acc: 0.979 - ETA: 1:07 - loss: 0.0643 - acc: 0.979 - ETA: 1:06 - loss: 0.0640 - acc: 0.979 - ETA: 1:04 - loss: 0.0639 - acc: 0.979 - ETA: 1:03 - loss: 0.0638 - acc: 0.979 - ETA: 1:02 - loss: 0.0635 - acc: 0.979 - ETA: 1:01 - loss: 0.0632 - acc: 0.979 - ETA: 59s - loss: 0.0632 - acc: 0.979 - ETA: 58s - loss: 0.0631 - acc: 0.97 - ETA: 57s - loss: 0.0628 - acc: 0.97 - ETA: 56s - loss: 0.0633 - acc: 0.97 - ETA: 55s - loss: 0.0634 - acc: 0.97 - ETA: 53s - loss: 0.0636 - acc: 0.97 - ETA: 52s - loss: 0.0636 - acc: 0.97 - ETA: 51s - loss: 0.0634 - acc: 0.97 - ETA: 50s - loss: 0.0639 - acc: 0.97 - ETA: 49s - loss: 0.0640 - acc: 0.97 - ETA: 48s - loss: 0.0642 - acc: 0.97 - ETA: 46s - loss: 0.0639 - acc: 0.97 - ETA: 45s - loss: 0.0640 - acc: 0.97 - ETA: 44s - loss: 0.0643 - acc: 0.97 - ETA: 42s - loss: 0.0643 - acc: 0.97 - ETA: 41s - loss: 0.0644 - acc: 0.97 - ETA: 40s - loss: 0.0648 - acc: 0.97 - ETA: 38s - loss: 0.0647 - acc: 0.97 - ETA: 37s - loss: 0.0648 - acc: 0.97 - ETA: 36s - loss: 0.0651 - acc: 0.97 - ETA: 34s - loss: 0.0649 - acc: 0.97 - ETA: 33s - loss: 0.0648 - acc: 0.97 - ETA: 32s - loss: 0.0653 - acc: 0.97 - ETA: 30s - loss: 0.0651 - acc: 0.97 - ETA: 29s - loss: 0.0652 - acc: 0.97 - ETA: 28s - loss: 0.0653 - acc: 0.97 - ETA: 26s - loss: 0.0653 - acc: 0.97 - ETA: 25s - loss: 0.0657 - acc: 0.97 - ETA: 24s - loss: 0.0657 - acc: 0.97 - ETA: 22s - loss: 0.0660 - acc: 0.97 - ETA: 21s - loss: 0.0663 - acc: 0.97 - ETA: 20s - loss: 0.0666 - acc: 0.97 - ETA: 18s - loss: 0.0666 - acc: 0.97 - ETA: 17s - loss: 0.0667 - acc: 0.97 - ETA: 16s - loss: 0.0668 - acc: 0.97 - ETA: 14s - loss: 0.0667 - acc: 0.97 - ETA: 13s - loss: 0.0669 - acc: 0.97 - ETA: 12s - loss: 0.0668 - acc: 0.97 - ETA: 11s - loss: 0.0669 - acc: 0.97 - ETA: 9s - loss: 0.0669 - acc: 0.9773 - ETA: 8s - loss: 0.0667 - acc: 0.977 - ETA: 7s - loss: 0.0668 - acc: 0.977 - ETA: 5s - loss: 0.0668 - acc: 0.977 - ETA: 4s - loss: 0.0670 - acc: 0.977 - ETA: 3s - loss: 0.0672 - acc: 0.977 - ETA: 1s - loss: 0.0672 - acc: 0.977 - ETA: 0s - loss: 0.0673 - acc: 0.977 - 154s 3ms/step - loss: 0.0673 - acc: 0.9770 - val_loss: 0.3329 - val_acc: 0.9026\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "# Initialising the model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Adding embedding\n",
    "model_1.add(Embedding(len(vocabulary), embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "# Adding Dropout\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "# Adding first LSTM layer\n",
    "model_1.add(LSTM(100))\n",
    "\n",
    "# Adding Dropout\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "# Adding output layer\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Printing the model summary\n",
    "print(model_1.summary())\n",
    "\n",
    "# Compiling the model\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the data to the model\n",
    "history_1 = model_1.fit(X_train, Y_train, nb_epoch=10, batch_size=512 ,verbose=1,validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fODbrF5zipmY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.26%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5ab063b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_1.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Test and train accuracy of the model\n",
    "model_1_test = scores[1]\n",
    "model_1_train = max(history_1.history['acc'])\n",
    "\n",
    "# Plotting Train and Test Loss VS no. of epochs\n",
    "# list of epoch numbers\n",
    "x = list(range(1,11))\n",
    "\n",
    "# Validation loss\n",
    "vy = history_1.history['val_loss']\n",
    "# Training loss\n",
    "ty = history_1.history['loss']\n",
    "\n",
    "# Calling the function to draw the plot\n",
    "plt_dynamic(x, vy, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qm5HIEWjqd4"
   },
   "source": [
    "<h2>(2). RNN with 2 LSTM layers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqRe5K0_ippv",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 32)           2232096   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100, 100)          53200     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,365,797\n",
      "Trainable params: 2,365,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/10\n",
      "56000/56000 [==============================] - ETA: 13:04 - loss: 0.6932 - acc: 0.50 - ETA: 10:05 - loss: 0.6907 - acc: 0.67 - ETA: 9:04 - loss: 0.6872 - acc: 0.7318 - ETA: 8:17 - loss: 0.6829 - acc: 0.758 - ETA: 7:52 - loss: 0.6774 - acc: 0.780 - ETA: 7:35 - loss: 0.6713 - acc: 0.788 - ETA: 7:25 - loss: 0.6629 - acc: 0.794 - ETA: 7:13 - loss: 0.6529 - acc: 0.797 - ETA: 7:01 - loss: 0.6391 - acc: 0.801 - ETA: 6:50 - loss: 0.6180 - acc: 0.807 - ETA: 6:40 - loss: 0.6026 - acc: 0.809 - ETA: 6:32 - loss: 0.5918 - acc: 0.813 - ETA: 6:24 - loss: 0.5846 - acc: 0.815 - ETA: 6:17 - loss: 0.5767 - acc: 0.817 - ETA: 6:11 - loss: 0.5704 - acc: 0.817 - ETA: 6:05 - loss: 0.5601 - acc: 0.821 - ETA: 5:59 - loss: 0.5548 - acc: 0.821 - ETA: 5:53 - loss: 0.5512 - acc: 0.820 - ETA: 5:47 - loss: 0.5445 - acc: 0.823 - ETA: 5:41 - loss: 0.5410 - acc: 0.823 - ETA: 5:36 - loss: 0.5374 - acc: 0.823 - ETA: 5:31 - loss: 0.5328 - acc: 0.825 - ETA: 5:26 - loss: 0.5299 - acc: 0.825 - ETA: 5:21 - loss: 0.5261 - acc: 0.825 - ETA: 5:16 - loss: 0.5218 - acc: 0.827 - ETA: 5:12 - loss: 0.5179 - acc: 0.828 - ETA: 5:08 - loss: 0.5164 - acc: 0.827 - ETA: 5:03 - loss: 0.5123 - acc: 0.828 - ETA: 4:59 - loss: 0.5106 - acc: 0.828 - ETA: 4:55 - loss: 0.5088 - acc: 0.829 - ETA: 4:51 - loss: 0.5084 - acc: 0.828 - ETA: 4:47 - loss: 0.5070 - acc: 0.828 - ETA: 4:43 - loss: 0.5054 - acc: 0.828 - ETA: 4:39 - loss: 0.5022 - acc: 0.829 - ETA: 4:35 - loss: 0.5021 - acc: 0.828 - ETA: 4:31 - loss: 0.5018 - acc: 0.827 - ETA: 4:26 - loss: 0.4994 - acc: 0.828 - ETA: 4:22 - loss: 0.4973 - acc: 0.828 - ETA: 4:18 - loss: 0.4952 - acc: 0.829 - ETA: 4:14 - loss: 0.4933 - acc: 0.829 - ETA: 4:10 - loss: 0.4911 - acc: 0.830 - ETA: 4:06 - loss: 0.4892 - acc: 0.830 - ETA: 4:02 - loss: 0.4880 - acc: 0.830 - ETA: 3:59 - loss: 0.4867 - acc: 0.830 - ETA: 3:55 - loss: 0.4853 - acc: 0.830 - ETA: 3:51 - loss: 0.4843 - acc: 0.830 - ETA: 3:47 - loss: 0.4823 - acc: 0.831 - ETA: 3:43 - loss: 0.4812 - acc: 0.830 - ETA: 3:39 - loss: 0.4805 - acc: 0.830 - ETA: 3:36 - loss: 0.4786 - acc: 0.831 - ETA: 3:32 - loss: 0.4770 - acc: 0.831 - ETA: 3:28 - loss: 0.4764 - acc: 0.830 - ETA: 3:24 - loss: 0.4744 - acc: 0.831 - ETA: 3:20 - loss: 0.4725 - acc: 0.831 - ETA: 3:17 - loss: 0.4711 - acc: 0.831 - ETA: 3:13 - loss: 0.4692 - acc: 0.832 - ETA: 3:09 - loss: 0.4675 - acc: 0.832 - ETA: 3:06 - loss: 0.4657 - acc: 0.833 - ETA: 3:02 - loss: 0.4636 - acc: 0.833 - ETA: 2:58 - loss: 0.4618 - acc: 0.833 - ETA: 2:54 - loss: 0.4599 - acc: 0.833 - ETA: 2:51 - loss: 0.4579 - acc: 0.834 - ETA: 2:47 - loss: 0.4557 - acc: 0.834 - ETA: 2:44 - loss: 0.4534 - acc: 0.835 - ETA: 2:40 - loss: 0.4516 - acc: 0.835 - ETA: 2:36 - loss: 0.4490 - acc: 0.836 - ETA: 2:32 - loss: 0.4470 - acc: 0.837 - ETA: 2:29 - loss: 0.4452 - acc: 0.837 - ETA: 2:25 - loss: 0.4435 - acc: 0.837 - ETA: 2:22 - loss: 0.4416 - acc: 0.838 - ETA: 2:18 - loss: 0.4399 - acc: 0.838 - ETA: 2:14 - loss: 0.4374 - acc: 0.839 - ETA: 2:11 - loss: 0.4356 - acc: 0.839 - ETA: 2:07 - loss: 0.4338 - acc: 0.840 - ETA: 2:04 - loss: 0.4311 - acc: 0.841 - ETA: 2:00 - loss: 0.4297 - acc: 0.841 - ETA: 1:56 - loss: 0.4276 - acc: 0.842 - ETA: 1:53 - loss: 0.4263 - acc: 0.842 - ETA: 1:49 - loss: 0.4242 - acc: 0.843 - ETA: 1:46 - loss: 0.4223 - acc: 0.843 - ETA: 1:42 - loss: 0.4212 - acc: 0.844 - ETA: 1:38 - loss: 0.4201 - acc: 0.844 - ETA: 1:35 - loss: 0.4182 - acc: 0.845 - ETA: 1:31 - loss: 0.4165 - acc: 0.845 - ETA: 1:28 - loss: 0.4150 - acc: 0.846 - ETA: 1:24 - loss: 0.4135 - acc: 0.846 - ETA: 1:21 - loss: 0.4117 - acc: 0.847 - ETA: 1:17 - loss: 0.4099 - acc: 0.847 - ETA: 1:13 - loss: 0.4085 - acc: 0.848 - ETA: 1:10 - loss: 0.4065 - acc: 0.848 - ETA: 1:06 - loss: 0.4049 - acc: 0.849 - ETA: 1:03 - loss: 0.4031 - acc: 0.849 - ETA: 59s - loss: 0.4018 - acc: 0.850 - ETA: 55s - loss: 0.4003 - acc: 0.85 - ETA: 52s - loss: 0.3985 - acc: 0.85 - ETA: 48s - loss: 0.3971 - acc: 0.85 - ETA: 45s - loss: 0.3958 - acc: 0.85 - ETA: 41s - loss: 0.3944 - acc: 0.85 - ETA: 37s - loss: 0.3931 - acc: 0.85 - ETA: 34s - loss: 0.3914 - acc: 0.85 - ETA: 30s - loss: 0.3902 - acc: 0.85 - ETA: 26s - loss: 0.3890 - acc: 0.85 - ETA: 23s - loss: 0.3875 - acc: 0.85 - ETA: 19s - loss: 0.3862 - acc: 0.85 - ETA: 15s - loss: 0.3848 - acc: 0.85 - ETA: 12s - loss: 0.3834 - acc: 0.85 - ETA: 8s - loss: 0.3822 - acc: 0.8557 - ETA: 5s - loss: 0.3808 - acc: 0.856 - ETA: 1s - loss: 0.3796 - acc: 0.856 - 440s 8ms/step - loss: 0.3789 - acc: 0.8566 - val_loss: 0.2329 - val_acc: 0.9055\n",
      "Epoch 2/10\n",
      "56000/56000 [==============================] - ETA: 7:16 - loss: 0.2451 - acc: 0.908 - ETA: 7:10 - loss: 0.2393 - acc: 0.906 - ETA: 6:57 - loss: 0.2340 - acc: 0.906 - ETA: 6:52 - loss: 0.2263 - acc: 0.907 - ETA: 6:44 - loss: 0.2213 - acc: 0.910 - ETA: 6:39 - loss: 0.2228 - acc: 0.908 - ETA: 6:42 - loss: 0.2293 - acc: 0.904 - ETA: 6:39 - loss: 0.2255 - acc: 0.906 - ETA: 6:34 - loss: 0.2269 - acc: 0.905 - ETA: 6:38 - loss: 0.2247 - acc: 0.906 - ETA: 6:38 - loss: 0.2235 - acc: 0.906 - ETA: 6:38 - loss: 0.2215 - acc: 0.908 - ETA: 6:37 - loss: 0.2210 - acc: 0.909 - ETA: 6:34 - loss: 0.2232 - acc: 0.908 - ETA: 6:32 - loss: 0.2197 - acc: 0.909 - ETA: 6:30 - loss: 0.2177 - acc: 0.910 - ETA: 6:28 - loss: 0.2190 - acc: 0.910 - ETA: 6:24 - loss: 0.2167 - acc: 0.911 - ETA: 6:21 - loss: 0.2141 - acc: 0.912 - ETA: 6:18 - loss: 0.2144 - acc: 0.911 - ETA: 6:15 - loss: 0.2138 - acc: 0.911 - ETA: 6:14 - loss: 0.2143 - acc: 0.911 - ETA: 6:10 - loss: 0.2130 - acc: 0.912 - ETA: 6:05 - loss: 0.2148 - acc: 0.912 - ETA: 6:00 - loss: 0.2140 - acc: 0.912 - ETA: 5:55 - loss: 0.2136 - acc: 0.912 - ETA: 5:50 - loss: 0.2139 - acc: 0.912 - ETA: 5:45 - loss: 0.2145 - acc: 0.912 - ETA: 5:40 - loss: 0.2147 - acc: 0.911 - ETA: 5:35 - loss: 0.2148 - acc: 0.912 - ETA: 5:31 - loss: 0.2154 - acc: 0.911 - ETA: 5:26 - loss: 0.2156 - acc: 0.912 - ETA: 5:21 - loss: 0.2170 - acc: 0.911 - ETA: 5:17 - loss: 0.2162 - acc: 0.912 - ETA: 5:13 - loss: 0.2164 - acc: 0.912 - ETA: 5:08 - loss: 0.2167 - acc: 0.912 - ETA: 5:04 - loss: 0.2168 - acc: 0.912 - ETA: 5:00 - loss: 0.2167 - acc: 0.912 - ETA: 4:55 - loss: 0.2169 - acc: 0.912 - ETA: 4:52 - loss: 0.2175 - acc: 0.912 - ETA: 4:48 - loss: 0.2177 - acc: 0.912 - ETA: 4:44 - loss: 0.2181 - acc: 0.911 - ETA: 4:40 - loss: 0.2181 - acc: 0.911 - ETA: 4:35 - loss: 0.2187 - acc: 0.911 - ETA: 4:31 - loss: 0.2190 - acc: 0.911 - ETA: 4:26 - loss: 0.2193 - acc: 0.911 - ETA: 4:21 - loss: 0.2193 - acc: 0.911 - ETA: 4:17 - loss: 0.2199 - acc: 0.910 - ETA: 4:13 - loss: 0.2198 - acc: 0.910 - ETA: 4:08 - loss: 0.2205 - acc: 0.910 - ETA: 4:04 - loss: 0.2194 - acc: 0.911 - ETA: 4:00 - loss: 0.2197 - acc: 0.911 - ETA: 3:55 - loss: 0.2193 - acc: 0.911 - ETA: 3:51 - loss: 0.2192 - acc: 0.911 - ETA: 3:47 - loss: 0.2187 - acc: 0.911 - ETA: 3:43 - loss: 0.2182 - acc: 0.912 - ETA: 3:39 - loss: 0.2178 - acc: 0.912 - ETA: 3:35 - loss: 0.2175 - acc: 0.912 - ETA: 3:31 - loss: 0.2172 - acc: 0.912 - ETA: 3:27 - loss: 0.2166 - acc: 0.913 - ETA: 3:23 - loss: 0.2164 - acc: 0.913 - ETA: 3:19 - loss: 0.2167 - acc: 0.912 - ETA: 3:14 - loss: 0.2163 - acc: 0.913 - ETA: 3:10 - loss: 0.2156 - acc: 0.913 - ETA: 3:06 - loss: 0.2152 - acc: 0.913 - ETA: 3:01 - loss: 0.2149 - acc: 0.914 - ETA: 2:57 - loss: 0.2144 - acc: 0.914 - ETA: 2:53 - loss: 0.2143 - acc: 0.914 - ETA: 2:49 - loss: 0.2141 - acc: 0.914 - ETA: 2:44 - loss: 0.2139 - acc: 0.914 - ETA: 2:40 - loss: 0.2142 - acc: 0.914 - ETA: 2:36 - loss: 0.2147 - acc: 0.914 - ETA: 2:32 - loss: 0.2147 - acc: 0.913 - ETA: 2:27 - loss: 0.2150 - acc: 0.913 - ETA: 2:23 - loss: 0.2156 - acc: 0.913 - ETA: 2:19 - loss: 0.2159 - acc: 0.913 - ETA: 2:15 - loss: 0.2159 - acc: 0.913 - ETA: 2:10 - loss: 0.2156 - acc: 0.913 - ETA: 2:06 - loss: 0.2156 - acc: 0.913 - ETA: 2:02 - loss: 0.2150 - acc: 0.913 - ETA: 1:58 - loss: 0.2153 - acc: 0.913 - ETA: 1:54 - loss: 0.2156 - acc: 0.913 - ETA: 1:49 - loss: 0.2158 - acc: 0.913 - ETA: 1:45 - loss: 0.2157 - acc: 0.913 - ETA: 1:41 - loss: 0.2155 - acc: 0.913 - ETA: 1:37 - loss: 0.2152 - acc: 0.913 - ETA: 1:33 - loss: 0.2150 - acc: 0.914 - ETA: 1:28 - loss: 0.2153 - acc: 0.913 - ETA: 1:24 - loss: 0.2151 - acc: 0.913 - ETA: 1:20 - loss: 0.2148 - acc: 0.914 - ETA: 1:16 - loss: 0.2147 - acc: 0.914 - ETA: 1:12 - loss: 0.2152 - acc: 0.914 - ETA: 1:08 - loss: 0.2151 - acc: 0.914 - ETA: 1:03 - loss: 0.2148 - acc: 0.914 - ETA: 59s - loss: 0.2148 - acc: 0.914 - ETA: 55s - loss: 0.2152 - acc: 0.91 - ETA: 51s - loss: 0.2149 - acc: 0.91 - ETA: 47s - loss: 0.2148 - acc: 0.91 - ETA: 43s - loss: 0.2147 - acc: 0.91 - ETA: 38s - loss: 0.2146 - acc: 0.91 - ETA: 34s - loss: 0.2149 - acc: 0.91 - ETA: 30s - loss: 0.2147 - acc: 0.91 - ETA: 26s - loss: 0.2143 - acc: 0.91 - ETA: 22s - loss: 0.2140 - acc: 0.91 - ETA: 18s - loss: 0.2139 - acc: 0.91 - ETA: 14s - loss: 0.2136 - acc: 0.91 - ETA: 9s - loss: 0.2136 - acc: 0.9151 - ETA: 5s - loss: 0.2137 - acc: 0.915 - ETA: 1s - loss: 0.2134 - acc: 0.915 - 485s 9ms/step - loss: 0.2133 - acc: 0.9151 - val_loss: 0.2172 - val_acc: 0.9149\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 7:49 - loss: 0.1654 - acc: 0.949 - ETA: 7:38 - loss: 0.1709 - acc: 0.938 - ETA: 7:27 - loss: 0.1682 - acc: 0.940 - ETA: 7:19 - loss: 0.1661 - acc: 0.939 - ETA: 7:12 - loss: 0.1719 - acc: 0.935 - ETA: 7:08 - loss: 0.1734 - acc: 0.934 - ETA: 7:03 - loss: 0.1696 - acc: 0.935 - ETA: 6:57 - loss: 0.1662 - acc: 0.937 - ETA: 6:51 - loss: 0.1683 - acc: 0.935 - ETA: 6:46 - loss: 0.1707 - acc: 0.934 - ETA: 6:42 - loss: 0.1711 - acc: 0.934 - ETA: 6:38 - loss: 0.1706 - acc: 0.934 - ETA: 6:33 - loss: 0.1696 - acc: 0.935 - ETA: 6:29 - loss: 0.1671 - acc: 0.936 - ETA: 6:26 - loss: 0.1665 - acc: 0.937 - ETA: 6:22 - loss: 0.1708 - acc: 0.935 - ETA: 6:18 - loss: 0.1705 - acc: 0.935 - ETA: 6:15 - loss: 0.1712 - acc: 0.934 - ETA: 6:11 - loss: 0.1694 - acc: 0.935 - ETA: 6:08 - loss: 0.1704 - acc: 0.934 - ETA: 6:04 - loss: 0.1709 - acc: 0.935 - ETA: 6:00 - loss: 0.1713 - acc: 0.934 - ETA: 5:56 - loss: 0.1732 - acc: 0.934 - ETA: 5:53 - loss: 0.1724 - acc: 0.934 - ETA: 5:49 - loss: 0.1718 - acc: 0.934 - ETA: 5:45 - loss: 0.1717 - acc: 0.934 - ETA: 5:42 - loss: 0.1702 - acc: 0.935 - ETA: 5:40 - loss: 0.1704 - acc: 0.935 - ETA: 5:37 - loss: 0.1702 - acc: 0.935 - ETA: 5:35 - loss: 0.1711 - acc: 0.934 - ETA: 5:31 - loss: 0.1699 - acc: 0.935 - ETA: 5:26 - loss: 0.1697 - acc: 0.935 - ETA: 5:22 - loss: 0.1695 - acc: 0.935 - ETA: 5:17 - loss: 0.1699 - acc: 0.935 - ETA: 5:13 - loss: 0.1695 - acc: 0.935 - ETA: 5:08 - loss: 0.1703 - acc: 0.935 - ETA: 5:04 - loss: 0.1710 - acc: 0.935 - ETA: 4:59 - loss: 0.1725 - acc: 0.934 - ETA: 4:55 - loss: 0.1721 - acc: 0.934 - ETA: 4:51 - loss: 0.1720 - acc: 0.934 - ETA: 4:46 - loss: 0.1719 - acc: 0.934 - ETA: 4:43 - loss: 0.1718 - acc: 0.934 - ETA: 4:39 - loss: 0.1716 - acc: 0.934 - ETA: 4:35 - loss: 0.1714 - acc: 0.934 - ETA: 4:30 - loss: 0.1711 - acc: 0.934 - ETA: 4:26 - loss: 0.1709 - acc: 0.935 - ETA: 4:22 - loss: 0.1710 - acc: 0.934 - ETA: 4:17 - loss: 0.1709 - acc: 0.934 - ETA: 4:13 - loss: 0.1713 - acc: 0.934 - ETA: 4:09 - loss: 0.1712 - acc: 0.934 - ETA: 4:05 - loss: 0.1710 - acc: 0.935 - ETA: 4:00 - loss: 0.1717 - acc: 0.934 - ETA: 3:56 - loss: 0.1712 - acc: 0.934 - ETA: 3:52 - loss: 0.1713 - acc: 0.934 - ETA: 3:48 - loss: 0.1712 - acc: 0.934 - ETA: 3:43 - loss: 0.1707 - acc: 0.934 - ETA: 3:39 - loss: 0.1710 - acc: 0.934 - ETA: 3:35 - loss: 0.1707 - acc: 0.934 - ETA: 3:30 - loss: 0.1711 - acc: 0.934 - ETA: 3:26 - loss: 0.1710 - acc: 0.934 - ETA: 3:22 - loss: 0.1718 - acc: 0.934 - ETA: 3:18 - loss: 0.1720 - acc: 0.934 - ETA: 3:13 - loss: 0.1717 - acc: 0.934 - ETA: 3:09 - loss: 0.1720 - acc: 0.934 - ETA: 3:05 - loss: 0.1722 - acc: 0.934 - ETA: 3:01 - loss: 0.1720 - acc: 0.934 - ETA: 2:57 - loss: 0.1725 - acc: 0.934 - ETA: 2:52 - loss: 0.1728 - acc: 0.934 - ETA: 2:48 - loss: 0.1724 - acc: 0.934 - ETA: 2:44 - loss: 0.1725 - acc: 0.934 - ETA: 2:40 - loss: 0.1724 - acc: 0.934 - ETA: 2:35 - loss: 0.1723 - acc: 0.934 - ETA: 2:31 - loss: 0.1723 - acc: 0.934 - ETA: 2:27 - loss: 0.1724 - acc: 0.934 - ETA: 2:23 - loss: 0.1729 - acc: 0.934 - ETA: 2:19 - loss: 0.1736 - acc: 0.933 - ETA: 2:14 - loss: 0.1735 - acc: 0.933 - ETA: 2:10 - loss: 0.1735 - acc: 0.933 - ETA: 2:06 - loss: 0.1739 - acc: 0.933 - ETA: 2:02 - loss: 0.1741 - acc: 0.933 - ETA: 1:58 - loss: 0.1741 - acc: 0.933 - ETA: 1:54 - loss: 0.1743 - acc: 0.933 - ETA: 1:49 - loss: 0.1737 - acc: 0.933 - ETA: 1:45 - loss: 0.1742 - acc: 0.933 - ETA: 1:41 - loss: 0.1744 - acc: 0.933 - ETA: 1:37 - loss: 0.1743 - acc: 0.933 - ETA: 1:33 - loss: 0.1742 - acc: 0.933 - ETA: 1:29 - loss: 0.1742 - acc: 0.933 - ETA: 1:24 - loss: 0.1741 - acc: 0.933 - ETA: 1:20 - loss: 0.1739 - acc: 0.933 - ETA: 1:16 - loss: 0.1739 - acc: 0.933 - ETA: 1:12 - loss: 0.1742 - acc: 0.933 - ETA: 1:08 - loss: 0.1743 - acc: 0.933 - ETA: 1:04 - loss: 0.1744 - acc: 0.933 - ETA: 1:00 - loss: 0.1743 - acc: 0.933 - ETA: 55s - loss: 0.1741 - acc: 0.933 - ETA: 51s - loss: 0.1744 - acc: 0.93 - ETA: 47s - loss: 0.1741 - acc: 0.93 - ETA: 43s - loss: 0.1738 - acc: 0.93 - ETA: 39s - loss: 0.1740 - acc: 0.93 - ETA: 35s - loss: 0.1744 - acc: 0.93 - ETA: 30s - loss: 0.1741 - acc: 0.93 - ETA: 26s - loss: 0.1742 - acc: 0.93 - ETA: 22s - loss: 0.1742 - acc: 0.93 - ETA: 18s - loss: 0.1739 - acc: 0.93 - ETA: 14s - loss: 0.1736 - acc: 0.93 - ETA: 10s - loss: 0.1739 - acc: 0.93 - ETA: 5s - loss: 0.1736 - acc: 0.9338 - ETA: 1s - loss: 0.1738 - acc: 0.933 - 491s 9ms/step - loss: 0.1735 - acc: 0.9339 - val_loss: 0.2207 - val_acc: 0.9156\n",
      "Epoch 4/10\n",
      "56000/56000 [==============================] - ETA: 7:40 - loss: 0.1486 - acc: 0.941 - ETA: 7:37 - loss: 0.1452 - acc: 0.942 - ETA: 7:28 - loss: 0.1540 - acc: 0.940 - ETA: 7:22 - loss: 0.1534 - acc: 0.942 - ETA: 7:16 - loss: 0.1471 - acc: 0.944 - ETA: 7:11 - loss: 0.1431 - acc: 0.946 - ETA: 7:08 - loss: 0.1450 - acc: 0.946 - ETA: 7:04 - loss: 0.1493 - acc: 0.944 - ETA: 7:00 - loss: 0.1470 - acc: 0.946 - ETA: 6:56 - loss: 0.1459 - acc: 0.946 - ETA: 6:56 - loss: 0.1492 - acc: 0.945 - ETA: 6:57 - loss: 0.1478 - acc: 0.945 - ETA: 6:56 - loss: 0.1459 - acc: 0.945 - ETA: 6:57 - loss: 0.1468 - acc: 0.944 - ETA: 6:57 - loss: 0.1454 - acc: 0.945 - ETA: 6:53 - loss: 0.1473 - acc: 0.944 - ETA: 6:49 - loss: 0.1487 - acc: 0.944 - ETA: 6:42 - loss: 0.1480 - acc: 0.944 - ETA: 6:37 - loss: 0.1489 - acc: 0.943 - ETA: 6:31 - loss: 0.1494 - acc: 0.943 - ETA: 6:26 - loss: 0.1492 - acc: 0.943 - ETA: 6:21 - loss: 0.1475 - acc: 0.944 - ETA: 6:16 - loss: 0.1473 - acc: 0.944 - ETA: 6:11 - loss: 0.1458 - acc: 0.945 - ETA: 6:07 - loss: 0.1483 - acc: 0.945 - ETA: 6:02 - loss: 0.1485 - acc: 0.944 - ETA: 5:57 - loss: 0.1479 - acc: 0.944 - ETA: 5:53 - loss: 0.1476 - acc: 0.944 - ETA: 5:48 - loss: 0.1471 - acc: 0.944 - ETA: 5:43 - loss: 0.1479 - acc: 0.944 - ETA: 5:39 - loss: 0.1494 - acc: 0.943 - ETA: 5:34 - loss: 0.1491 - acc: 0.943 - ETA: 5:30 - loss: 0.1478 - acc: 0.944 - ETA: 5:26 - loss: 0.1481 - acc: 0.944 - ETA: 5:21 - loss: 0.1478 - acc: 0.944 - ETA: 5:17 - loss: 0.1478 - acc: 0.944 - ETA: 5:13 - loss: 0.1482 - acc: 0.943 - ETA: 5:08 - loss: 0.1480 - acc: 0.943 - ETA: 5:04 - loss: 0.1481 - acc: 0.943 - ETA: 5:00 - loss: 0.1479 - acc: 0.943 - ETA: 4:55 - loss: 0.1476 - acc: 0.943 - ETA: 4:51 - loss: 0.1479 - acc: 0.943 - ETA: 4:47 - loss: 0.1477 - acc: 0.944 - ETA: 4:42 - loss: 0.1466 - acc: 0.944 - ETA: 4:38 - loss: 0.1453 - acc: 0.945 - ETA: 4:34 - loss: 0.1449 - acc: 0.945 - ETA: 4:30 - loss: 0.1447 - acc: 0.945 - ETA: 4:25 - loss: 0.1451 - acc: 0.945 - ETA: 4:21 - loss: 0.1448 - acc: 0.945 - ETA: 4:16 - loss: 0.1458 - acc: 0.945 - ETA: 4:12 - loss: 0.1460 - acc: 0.945 - ETA: 4:07 - loss: 0.1464 - acc: 0.945 - ETA: 4:02 - loss: 0.1463 - acc: 0.945 - ETA: 3:58 - loss: 0.1468 - acc: 0.945 - ETA: 3:53 - loss: 0.1471 - acc: 0.944 - ETA: 3:49 - loss: 0.1475 - acc: 0.944 - ETA: 3:45 - loss: 0.1471 - acc: 0.944 - ETA: 3:40 - loss: 0.1483 - acc: 0.944 - ETA: 3:36 - loss: 0.1481 - acc: 0.944 - ETA: 3:31 - loss: 0.1486 - acc: 0.944 - ETA: 3:27 - loss: 0.1486 - acc: 0.944 - ETA: 3:23 - loss: 0.1490 - acc: 0.944 - ETA: 3:18 - loss: 0.1497 - acc: 0.944 - ETA: 3:14 - loss: 0.1496 - acc: 0.944 - ETA: 3:09 - loss: 0.1494 - acc: 0.944 - ETA: 3:05 - loss: 0.1493 - acc: 0.944 - ETA: 3:01 - loss: 0.1494 - acc: 0.944 - ETA: 2:56 - loss: 0.1493 - acc: 0.944 - ETA: 2:52 - loss: 0.1492 - acc: 0.944 - ETA: 2:48 - loss: 0.1496 - acc: 0.944 - ETA: 2:43 - loss: 0.1501 - acc: 0.943 - ETA: 2:39 - loss: 0.1502 - acc: 0.943 - ETA: 2:35 - loss: 0.1504 - acc: 0.943 - ETA: 2:31 - loss: 0.1506 - acc: 0.943 - ETA: 2:26 - loss: 0.1508 - acc: 0.943 - ETA: 2:22 - loss: 0.1512 - acc: 0.943 - ETA: 2:18 - loss: 0.1517 - acc: 0.942 - ETA: 2:14 - loss: 0.1516 - acc: 0.942 - ETA: 2:09 - loss: 0.1522 - acc: 0.942 - ETA: 2:05 - loss: 0.1524 - acc: 0.942 - ETA: 2:01 - loss: 0.1530 - acc: 0.942 - ETA: 1:56 - loss: 0.1531 - acc: 0.942 - ETA: 1:52 - loss: 0.1533 - acc: 0.942 - ETA: 1:48 - loss: 0.1535 - acc: 0.942 - ETA: 1:43 - loss: 0.1534 - acc: 0.942 - ETA: 1:39 - loss: 0.1535 - acc: 0.942 - ETA: 1:35 - loss: 0.1537 - acc: 0.942 - ETA: 1:30 - loss: 0.1536 - acc: 0.942 - ETA: 1:26 - loss: 0.1534 - acc: 0.942 - ETA: 1:22 - loss: 0.1535 - acc: 0.942 - ETA: 1:18 - loss: 0.1536 - acc: 0.942 - ETA: 1:13 - loss: 0.1532 - acc: 0.942 - ETA: 1:09 - loss: 0.1531 - acc: 0.942 - ETA: 1:05 - loss: 0.1531 - acc: 0.942 - ETA: 1:01 - loss: 0.1528 - acc: 0.942 - ETA: 56s - loss: 0.1528 - acc: 0.942 - ETA: 52s - loss: 0.1528 - acc: 0.94 - ETA: 48s - loss: 0.1525 - acc: 0.94 - ETA: 44s - loss: 0.1525 - acc: 0.94 - ETA: 39s - loss: 0.1523 - acc: 0.94 - ETA: 35s - loss: 0.1522 - acc: 0.94 - ETA: 31s - loss: 0.1522 - acc: 0.94 - ETA: 27s - loss: 0.1517 - acc: 0.94 - ETA: 22s - loss: 0.1519 - acc: 0.94 - ETA: 18s - loss: 0.1519 - acc: 0.94 - ETA: 14s - loss: 0.1519 - acc: 0.94 - ETA: 10s - loss: 0.1515 - acc: 0.94 - ETA: 5s - loss: 0.1519 - acc: 0.9428 - ETA: 1s - loss: 0.1519 - acc: 0.942 - 495s 9ms/step - loss: 0.1517 - acc: 0.9428 - val_loss: 0.2256 - val_acc: 0.9181\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 10:07 - loss: 0.1110 - acc: 0.95 - ETA: 8:50 - loss: 0.1145 - acc: 0.9551 - ETA: 8:20 - loss: 0.1278 - acc: 0.951 - ETA: 8:04 - loss: 0.1320 - acc: 0.948 - ETA: 7:51 - loss: 0.1301 - acc: 0.950 - ETA: 7:42 - loss: 0.1297 - acc: 0.948 - ETA: 7:31 - loss: 0.1369 - acc: 0.947 - ETA: 7:23 - loss: 0.1358 - acc: 0.949 - ETA: 7:15 - loss: 0.1342 - acc: 0.951 - ETA: 7:09 - loss: 0.1331 - acc: 0.950 - ETA: 7:03 - loss: 0.1309 - acc: 0.951 - ETA: 6:56 - loss: 0.1277 - acc: 0.953 - ETA: 6:51 - loss: 0.1259 - acc: 0.954 - ETA: 6:45 - loss: 0.1268 - acc: 0.953 - ETA: 6:40 - loss: 0.1260 - acc: 0.953 - ETA: 6:35 - loss: 0.1256 - acc: 0.953 - ETA: 6:30 - loss: 0.1242 - acc: 0.953 - ETA: 6:27 - loss: 0.1242 - acc: 0.953 - ETA: 6:22 - loss: 0.1241 - acc: 0.953 - ETA: 6:26 - loss: 0.1258 - acc: 0.953 - ETA: 6:39 - loss: 0.1256 - acc: 0.953 - ETA: 6:42 - loss: 0.1267 - acc: 0.953 - ETA: 6:40 - loss: 0.1273 - acc: 0.953 - ETA: 6:36 - loss: 0.1282 - acc: 0.952 - ETA: 6:30 - loss: 0.1289 - acc: 0.952 - ETA: 6:24 - loss: 0.1274 - acc: 0.952 - ETA: 6:18 - loss: 0.1272 - acc: 0.953 - ETA: 6:12 - loss: 0.1279 - acc: 0.952 - ETA: 6:06 - loss: 0.1282 - acc: 0.952 - ETA: 6:01 - loss: 0.1290 - acc: 0.952 - ETA: 5:55 - loss: 0.1289 - acc: 0.952 - ETA: 5:50 - loss: 0.1279 - acc: 0.952 - ETA: 5:44 - loss: 0.1267 - acc: 0.953 - ETA: 5:39 - loss: 0.1270 - acc: 0.953 - ETA: 5:34 - loss: 0.1277 - acc: 0.952 - ETA: 5:29 - loss: 0.1274 - acc: 0.952 - ETA: 5:24 - loss: 0.1268 - acc: 0.953 - ETA: 5:19 - loss: 0.1271 - acc: 0.953 - ETA: 5:14 - loss: 0.1268 - acc: 0.953 - ETA: 5:09 - loss: 0.1275 - acc: 0.952 - ETA: 5:04 - loss: 0.1286 - acc: 0.952 - ETA: 5:00 - loss: 0.1284 - acc: 0.952 - ETA: 4:55 - loss: 0.1275 - acc: 0.953 - ETA: 4:51 - loss: 0.1283 - acc: 0.952 - ETA: 4:46 - loss: 0.1280 - acc: 0.952 - ETA: 4:41 - loss: 0.1275 - acc: 0.952 - ETA: 4:36 - loss: 0.1271 - acc: 0.953 - ETA: 4:31 - loss: 0.1267 - acc: 0.952 - ETA: 4:26 - loss: 0.1269 - acc: 0.952 - ETA: 4:21 - loss: 0.1268 - acc: 0.953 - ETA: 4:17 - loss: 0.1272 - acc: 0.952 - ETA: 4:12 - loss: 0.1274 - acc: 0.952 - ETA: 4:07 - loss: 0.1277 - acc: 0.952 - ETA: 4:03 - loss: 0.1281 - acc: 0.952 - ETA: 3:58 - loss: 0.1279 - acc: 0.952 - ETA: 3:54 - loss: 0.1278 - acc: 0.952 - ETA: 3:49 - loss: 0.1282 - acc: 0.952 - ETA: 3:44 - loss: 0.1281 - acc: 0.952 - ETA: 3:40 - loss: 0.1279 - acc: 0.952 - ETA: 3:35 - loss: 0.1283 - acc: 0.951 - ETA: 3:31 - loss: 0.1286 - acc: 0.951 - ETA: 3:26 - loss: 0.1288 - acc: 0.951 - ETA: 3:22 - loss: 0.1288 - acc: 0.951 - ETA: 3:17 - loss: 0.1287 - acc: 0.951 - ETA: 3:13 - loss: 0.1292 - acc: 0.951 - ETA: 3:08 - loss: 0.1293 - acc: 0.951 - ETA: 3:04 - loss: 0.1296 - acc: 0.951 - ETA: 2:59 - loss: 0.1294 - acc: 0.951 - ETA: 2:55 - loss: 0.1295 - acc: 0.951 - ETA: 2:50 - loss: 0.1298 - acc: 0.951 - ETA: 2:46 - loss: 0.1300 - acc: 0.951 - ETA: 2:41 - loss: 0.1295 - acc: 0.951 - ETA: 2:37 - loss: 0.1295 - acc: 0.951 - ETA: 2:32 - loss: 0.1297 - acc: 0.951 - ETA: 2:28 - loss: 0.1297 - acc: 0.951 - ETA: 2:24 - loss: 0.1300 - acc: 0.951 - ETA: 2:19 - loss: 0.1298 - acc: 0.951 - ETA: 2:15 - loss: 0.1302 - acc: 0.951 - ETA: 2:10 - loss: 0.1309 - acc: 0.950 - ETA: 2:06 - loss: 0.1312 - acc: 0.950 - ETA: 2:02 - loss: 0.1313 - acc: 0.950 - ETA: 1:57 - loss: 0.1313 - acc: 0.950 - ETA: 1:53 - loss: 0.1317 - acc: 0.950 - ETA: 1:48 - loss: 0.1318 - acc: 0.950 - ETA: 1:44 - loss: 0.1318 - acc: 0.950 - ETA: 1:40 - loss: 0.1314 - acc: 0.950 - ETA: 1:35 - loss: 0.1318 - acc: 0.950 - ETA: 1:31 - loss: 0.1320 - acc: 0.950 - ETA: 1:27 - loss: 0.1323 - acc: 0.950 - ETA: 1:23 - loss: 0.1326 - acc: 0.950 - ETA: 1:18 - loss: 0.1326 - acc: 0.950 - ETA: 1:14 - loss: 0.1328 - acc: 0.950 - ETA: 1:10 - loss: 0.1329 - acc: 0.950 - ETA: 1:05 - loss: 0.1328 - acc: 0.950 - ETA: 1:01 - loss: 0.1328 - acc: 0.950 - ETA: 57s - loss: 0.1326 - acc: 0.950 - ETA: 52s - loss: 0.1326 - acc: 0.95 - ETA: 48s - loss: 0.1325 - acc: 0.95 - ETA: 44s - loss: 0.1329 - acc: 0.95 - ETA: 40s - loss: 0.1326 - acc: 0.95 - ETA: 35s - loss: 0.1326 - acc: 0.95 - ETA: 31s - loss: 0.1326 - acc: 0.95 - ETA: 27s - loss: 0.1329 - acc: 0.95 - ETA: 22s - loss: 0.1332 - acc: 0.95 - ETA: 18s - loss: 0.1329 - acc: 0.95 - ETA: 14s - loss: 0.1328 - acc: 0.95 - ETA: 10s - loss: 0.1325 - acc: 0.95 - ETA: 5s - loss: 0.1326 - acc: 0.9504 - ETA: 1s - loss: 0.1331 - acc: 0.950 - 497s 9ms/step - loss: 0.1331 - acc: 0.9503 - val_loss: 0.2407 - val_acc: 0.9125\n",
      "Epoch 6/10\n",
      "56000/56000 [==============================] - ETA: 8:58 - loss: 0.1121 - acc: 0.964 - ETA: 8:17 - loss: 0.1214 - acc: 0.958 - ETA: 8:18 - loss: 0.1134 - acc: 0.959 - ETA: 8:10 - loss: 0.1067 - acc: 0.961 - ETA: 8:08 - loss: 0.1041 - acc: 0.963 - ETA: 8:13 - loss: 0.1009 - acc: 0.964 - ETA: 8:13 - loss: 0.0995 - acc: 0.964 - ETA: 8:05 - loss: 0.1015 - acc: 0.963 - ETA: 8:05 - loss: 0.1067 - acc: 0.961 - ETA: 8:00 - loss: 0.1058 - acc: 0.960 - ETA: 7:54 - loss: 0.1046 - acc: 0.961 - ETA: 7:51 - loss: 0.1042 - acc: 0.961 - ETA: 7:44 - loss: 0.1025 - acc: 0.962 - ETA: 7:38 - loss: 0.1060 - acc: 0.962 - ETA: 7:32 - loss: 0.1079 - acc: 0.961 - ETA: 7:26 - loss: 0.1085 - acc: 0.960 - ETA: 7:20 - loss: 0.1084 - acc: 0.960 - ETA: 7:14 - loss: 0.1103 - acc: 0.959 - ETA: 7:12 - loss: 0.1104 - acc: 0.959 - ETA: 7:09 - loss: 0.1106 - acc: 0.959 - ETA: 7:06 - loss: 0.1111 - acc: 0.959 - ETA: 7:01 - loss: 0.1112 - acc: 0.959 - ETA: 6:58 - loss: 0.1109 - acc: 0.959 - ETA: 6:53 - loss: 0.1101 - acc: 0.959 - ETA: 6:47 - loss: 0.1118 - acc: 0.959 - ETA: 6:42 - loss: 0.1130 - acc: 0.958 - ETA: 6:36 - loss: 0.1128 - acc: 0.958 - ETA: 6:32 - loss: 0.1130 - acc: 0.958 - ETA: 6:28 - loss: 0.1136 - acc: 0.957 - ETA: 6:25 - loss: 0.1140 - acc: 0.957 - ETA: 6:20 - loss: 0.1135 - acc: 0.958 - ETA: 6:16 - loss: 0.1145 - acc: 0.957 - ETA: 6:13 - loss: 0.1155 - acc: 0.957 - ETA: 6:08 - loss: 0.1154 - acc: 0.957 - ETA: 6:03 - loss: 0.1149 - acc: 0.958 - ETA: 5:59 - loss: 0.1143 - acc: 0.958 - ETA: 5:54 - loss: 0.1143 - acc: 0.958 - ETA: 5:50 - loss: 0.1150 - acc: 0.957 - ETA: 5:47 - loss: 0.1155 - acc: 0.957 - ETA: 5:44 - loss: 0.1160 - acc: 0.957 - ETA: 5:39 - loss: 0.1165 - acc: 0.957 - ETA: 5:34 - loss: 0.1168 - acc: 0.957 - ETA: 5:28 - loss: 0.1174 - acc: 0.957 - ETA: 5:22 - loss: 0.1184 - acc: 0.957 - ETA: 5:17 - loss: 0.1182 - acc: 0.956 - ETA: 5:11 - loss: 0.1183 - acc: 0.957 - ETA: 5:05 - loss: 0.1188 - acc: 0.956 - ETA: 5:00 - loss: 0.1196 - acc: 0.956 - ETA: 4:54 - loss: 0.1190 - acc: 0.956 - ETA: 4:49 - loss: 0.1194 - acc: 0.956 - ETA: 4:44 - loss: 0.1198 - acc: 0.956 - ETA: 4:38 - loss: 0.1197 - acc: 0.956 - ETA: 4:32 - loss: 0.1203 - acc: 0.956 - ETA: 4:27 - loss: 0.1207 - acc: 0.955 - ETA: 4:22 - loss: 0.1206 - acc: 0.956 - ETA: 4:16 - loss: 0.1207 - acc: 0.956 - ETA: 4:11 - loss: 0.1210 - acc: 0.955 - ETA: 4:06 - loss: 0.1209 - acc: 0.955 - ETA: 4:01 - loss: 0.1207 - acc: 0.955 - ETA: 3:56 - loss: 0.1204 - acc: 0.955 - ETA: 3:51 - loss: 0.1208 - acc: 0.955 - ETA: 3:46 - loss: 0.1207 - acc: 0.955 - ETA: 3:41 - loss: 0.1207 - acc: 0.955 - ETA: 3:36 - loss: 0.1209 - acc: 0.955 - ETA: 3:31 - loss: 0.1209 - acc: 0.955 - ETA: 3:26 - loss: 0.1204 - acc: 0.955 - ETA: 3:22 - loss: 0.1203 - acc: 0.955 - ETA: 3:17 - loss: 0.1201 - acc: 0.955 - ETA: 3:12 - loss: 0.1200 - acc: 0.956 - ETA: 3:08 - loss: 0.1203 - acc: 0.955 - ETA: 3:03 - loss: 0.1206 - acc: 0.955 - ETA: 2:58 - loss: 0.1210 - acc: 0.955 - ETA: 2:53 - loss: 0.1208 - acc: 0.955 - ETA: 2:49 - loss: 0.1205 - acc: 0.955 - ETA: 2:44 - loss: 0.1206 - acc: 0.955 - ETA: 2:39 - loss: 0.1206 - acc: 0.955 - ETA: 2:34 - loss: 0.1211 - acc: 0.955 - ETA: 2:29 - loss: 0.1215 - acc: 0.955 - ETA: 2:24 - loss: 0.1210 - acc: 0.955 - ETA: 2:20 - loss: 0.1209 - acc: 0.955 - ETA: 2:15 - loss: 0.1208 - acc: 0.955 - ETA: 2:10 - loss: 0.1209 - acc: 0.955 - ETA: 2:05 - loss: 0.1209 - acc: 0.955 - ETA: 2:00 - loss: 0.1209 - acc: 0.955 - ETA: 1:55 - loss: 0.1214 - acc: 0.955 - ETA: 1:50 - loss: 0.1213 - acc: 0.955 - ETA: 1:46 - loss: 0.1214 - acc: 0.955 - ETA: 1:41 - loss: 0.1214 - acc: 0.955 - ETA: 1:36 - loss: 0.1213 - acc: 0.955 - ETA: 1:31 - loss: 0.1214 - acc: 0.955 - ETA: 1:26 - loss: 0.1214 - acc: 0.955 - ETA: 1:22 - loss: 0.1219 - acc: 0.954 - ETA: 1:17 - loss: 0.1217 - acc: 0.954 - ETA: 1:12 - loss: 0.1218 - acc: 0.954 - ETA: 1:07 - loss: 0.1215 - acc: 0.955 - ETA: 1:02 - loss: 0.1221 - acc: 0.954 - ETA: 58s - loss: 0.1222 - acc: 0.954 - ETA: 53s - loss: 0.1221 - acc: 0.95 - ETA: 48s - loss: 0.1222 - acc: 0.95 - ETA: 43s - loss: 0.1223 - acc: 0.95 - ETA: 39s - loss: 0.1222 - acc: 0.95 - ETA: 34s - loss: 0.1222 - acc: 0.95 - ETA: 29s - loss: 0.1225 - acc: 0.95 - ETA: 25s - loss: 0.1225 - acc: 0.95 - ETA: 20s - loss: 0.1224 - acc: 0.95 - ETA: 15s - loss: 0.1224 - acc: 0.95 - ETA: 11s - loss: 0.1221 - acc: 0.95 - ETA: 6s - loss: 0.1219 - acc: 0.9549 - ETA: 1s - loss: 0.1222 - acc: 0.954 - 542s 10ms/step - loss: 0.1223 - acc: 0.9546 - val_loss: 0.2539 - val_acc: 0.9102\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 8:17 - loss: 0.0999 - acc: 0.962 - ETA: 7:59 - loss: 0.1042 - acc: 0.963 - ETA: 7:45 - loss: 0.1045 - acc: 0.966 - ETA: 7:55 - loss: 0.1019 - acc: 0.966 - ETA: 7:56 - loss: 0.1052 - acc: 0.964 - ETA: 7:58 - loss: 0.1033 - acc: 0.963 - ETA: 7:54 - loss: 0.1033 - acc: 0.963 - ETA: 7:45 - loss: 0.1013 - acc: 0.964 - ETA: 7:38 - loss: 0.1019 - acc: 0.963 - ETA: 7:31 - loss: 0.1008 - acc: 0.963 - ETA: 7:25 - loss: 0.0995 - acc: 0.964 - ETA: 7:18 - loss: 0.0994 - acc: 0.964 - ETA: 7:12 - loss: 0.0978 - acc: 0.964 - ETA: 7:05 - loss: 0.0981 - acc: 0.964 - ETA: 6:59 - loss: 0.0983 - acc: 0.963 - ETA: 6:53 - loss: 0.0989 - acc: 0.963 - ETA: 6:47 - loss: 0.1006 - acc: 0.963 - ETA: 6:41 - loss: 0.1012 - acc: 0.963 - ETA: 6:36 - loss: 0.1014 - acc: 0.963 - ETA: 6:32 - loss: 0.1000 - acc: 0.964 - ETA: 6:31 - loss: 0.0996 - acc: 0.964 - ETA: 6:26 - loss: 0.0987 - acc: 0.964 - ETA: 6:21 - loss: 0.0981 - acc: 0.965 - ETA: 6:19 - loss: 0.0981 - acc: 0.966 - ETA: 6:16 - loss: 0.0987 - acc: 0.965 - ETA: 6:14 - loss: 0.0979 - acc: 0.966 - ETA: 6:13 - loss: 0.0982 - acc: 0.965 - ETA: 6:09 - loss: 0.0998 - acc: 0.965 - ETA: 6:05 - loss: 0.0994 - acc: 0.965 - ETA: 6:03 - loss: 0.0993 - acc: 0.965 - ETA: 5:59 - loss: 0.0994 - acc: 0.965 - ETA: 5:55 - loss: 0.0998 - acc: 0.965 - ETA: 5:51 - loss: 0.0997 - acc: 0.965 - ETA: 5:47 - loss: 0.0993 - acc: 0.965 - ETA: 5:43 - loss: 0.1004 - acc: 0.964 - ETA: 5:38 - loss: 0.1001 - acc: 0.964 - ETA: 5:34 - loss: 0.1004 - acc: 0.964 - ETA: 5:30 - loss: 0.1010 - acc: 0.964 - ETA: 5:25 - loss: 0.1011 - acc: 0.964 - ETA: 5:21 - loss: 0.1009 - acc: 0.964 - ETA: 5:17 - loss: 0.1022 - acc: 0.964 - ETA: 5:14 - loss: 0.1030 - acc: 0.964 - ETA: 5:12 - loss: 0.1026 - acc: 0.964 - ETA: 5:08 - loss: 0.1024 - acc: 0.964 - ETA: 5:04 - loss: 0.1023 - acc: 0.964 - ETA: 5:00 - loss: 0.1027 - acc: 0.964 - ETA: 4:56 - loss: 0.1033 - acc: 0.963 - ETA: 4:52 - loss: 0.1033 - acc: 0.963 - ETA: 4:47 - loss: 0.1026 - acc: 0.963 - ETA: 4:43 - loss: 0.1023 - acc: 0.964 - ETA: 4:40 - loss: 0.1025 - acc: 0.964 - ETA: 4:36 - loss: 0.1038 - acc: 0.963 - ETA: 4:31 - loss: 0.1044 - acc: 0.963 - ETA: 4:27 - loss: 0.1043 - acc: 0.963 - ETA: 4:22 - loss: 0.1042 - acc: 0.963 - ETA: 4:17 - loss: 0.1045 - acc: 0.962 - ETA: 4:12 - loss: 0.1050 - acc: 0.962 - ETA: 4:08 - loss: 0.1051 - acc: 0.962 - ETA: 4:03 - loss: 0.1055 - acc: 0.962 - ETA: 3:59 - loss: 0.1056 - acc: 0.962 - ETA: 3:54 - loss: 0.1052 - acc: 0.962 - ETA: 3:49 - loss: 0.1049 - acc: 0.962 - ETA: 3:44 - loss: 0.1053 - acc: 0.962 - ETA: 3:39 - loss: 0.1058 - acc: 0.962 - ETA: 3:35 - loss: 0.1060 - acc: 0.962 - ETA: 3:30 - loss: 0.1061 - acc: 0.962 - ETA: 3:25 - loss: 0.1059 - acc: 0.962 - ETA: 3:20 - loss: 0.1058 - acc: 0.962 - ETA: 3:15 - loss: 0.1063 - acc: 0.962 - ETA: 3:10 - loss: 0.1063 - acc: 0.962 - ETA: 3:05 - loss: 0.1063 - acc: 0.962 - ETA: 3:00 - loss: 0.1066 - acc: 0.961 - ETA: 2:56 - loss: 0.1069 - acc: 0.961 - ETA: 2:51 - loss: 0.1065 - acc: 0.961 - ETA: 2:46 - loss: 0.1064 - acc: 0.962 - ETA: 2:41 - loss: 0.1064 - acc: 0.962 - ETA: 2:36 - loss: 0.1065 - acc: 0.962 - ETA: 2:32 - loss: 0.1063 - acc: 0.962 - ETA: 2:27 - loss: 0.1064 - acc: 0.962 - ETA: 2:22 - loss: 0.1069 - acc: 0.961 - ETA: 2:17 - loss: 0.1070 - acc: 0.961 - ETA: 2:12 - loss: 0.1070 - acc: 0.962 - ETA: 2:07 - loss: 0.1069 - acc: 0.962 - ETA: 2:02 - loss: 0.1070 - acc: 0.962 - ETA: 1:57 - loss: 0.1070 - acc: 0.962 - ETA: 1:52 - loss: 0.1072 - acc: 0.962 - ETA: 1:48 - loss: 0.1077 - acc: 0.962 - ETA: 1:43 - loss: 0.1075 - acc: 0.962 - ETA: 1:38 - loss: 0.1076 - acc: 0.962 - ETA: 1:33 - loss: 0.1077 - acc: 0.962 - ETA: 1:28 - loss: 0.1079 - acc: 0.962 - ETA: 1:23 - loss: 0.1083 - acc: 0.962 - ETA: 1:18 - loss: 0.1082 - acc: 0.961 - ETA: 1:14 - loss: 0.1082 - acc: 0.961 - ETA: 1:09 - loss: 0.1081 - acc: 0.961 - ETA: 1:04 - loss: 0.1084 - acc: 0.961 - ETA: 59s - loss: 0.1087 - acc: 0.961 - ETA: 54s - loss: 0.1089 - acc: 0.96 - ETA: 50s - loss: 0.1088 - acc: 0.96 - ETA: 45s - loss: 0.1092 - acc: 0.96 - ETA: 40s - loss: 0.1095 - acc: 0.96 - ETA: 35s - loss: 0.1097 - acc: 0.96 - ETA: 30s - loss: 0.1096 - acc: 0.96 - ETA: 25s - loss: 0.1100 - acc: 0.96 - ETA: 21s - loss: 0.1099 - acc: 0.96 - ETA: 16s - loss: 0.1103 - acc: 0.96 - ETA: 11s - loss: 0.1103 - acc: 0.96 - ETA: 6s - loss: 0.1104 - acc: 0.9611 - ETA: 1s - loss: 0.1106 - acc: 0.960 - 559s 10ms/step - loss: 0.1107 - acc: 0.9609 - val_loss: 0.2487 - val_acc: 0.9125\n",
      "Epoch 8/10\n",
      "56000/56000 [==============================] - ETA: 10:09 - loss: 0.0818 - acc: 0.97 - ETA: 9:28 - loss: 0.0813 - acc: 0.9727 - ETA: 9:00 - loss: 0.0917 - acc: 0.965 - ETA: 8:43 - loss: 0.0943 - acc: 0.966 - ETA: 8:30 - loss: 0.0943 - acc: 0.968 - ETA: 8:19 - loss: 0.0950 - acc: 0.967 - ETA: 8:08 - loss: 0.0957 - acc: 0.967 - ETA: 8:01 - loss: 0.0928 - acc: 0.968 - ETA: 7:53 - loss: 0.0963 - acc: 0.967 - ETA: 7:46 - loss: 0.0955 - acc: 0.966 - ETA: 7:39 - loss: 0.0948 - acc: 0.967 - ETA: 7:32 - loss: 0.0936 - acc: 0.968 - ETA: 7:26 - loss: 0.0941 - acc: 0.968 - ETA: 7:21 - loss: 0.0958 - acc: 0.967 - ETA: 7:15 - loss: 0.0973 - acc: 0.966 - ETA: 7:09 - loss: 0.0973 - acc: 0.967 - ETA: 7:04 - loss: 0.0984 - acc: 0.966 - ETA: 6:59 - loss: 0.0976 - acc: 0.966 - ETA: 6:55 - loss: 0.0976 - acc: 0.966 - ETA: 6:50 - loss: 0.0976 - acc: 0.966 - ETA: 6:46 - loss: 0.0968 - acc: 0.966 - ETA: 6:42 - loss: 0.0968 - acc: 0.966 - ETA: 6:37 - loss: 0.0956 - acc: 0.966 - ETA: 6:33 - loss: 0.0964 - acc: 0.966 - ETA: 6:29 - loss: 0.0969 - acc: 0.966 - ETA: 6:24 - loss: 0.0975 - acc: 0.966 - ETA: 6:20 - loss: 0.0976 - acc: 0.965 - ETA: 6:16 - loss: 0.0973 - acc: 0.965 - ETA: 6:12 - loss: 0.0963 - acc: 0.966 - ETA: 6:06 - loss: 0.0963 - acc: 0.966 - ETA: 6:01 - loss: 0.0962 - acc: 0.966 - ETA: 5:55 - loss: 0.0963 - acc: 0.966 - ETA: 5:50 - loss: 0.0960 - acc: 0.966 - ETA: 5:45 - loss: 0.0966 - acc: 0.965 - ETA: 5:40 - loss: 0.0965 - acc: 0.965 - ETA: 5:34 - loss: 0.0968 - acc: 0.965 - ETA: 5:29 - loss: 0.0968 - acc: 0.965 - ETA: 5:25 - loss: 0.0973 - acc: 0.965 - ETA: 5:20 - loss: 0.0977 - acc: 0.965 - ETA: 5:16 - loss: 0.0978 - acc: 0.965 - ETA: 5:11 - loss: 0.0972 - acc: 0.965 - ETA: 5:07 - loss: 0.0973 - acc: 0.965 - ETA: 5:02 - loss: 0.0967 - acc: 0.965 - ETA: 4:58 - loss: 0.0969 - acc: 0.965 - ETA: 4:53 - loss: 0.0978 - acc: 0.965 - ETA: 4:48 - loss: 0.0976 - acc: 0.964 - ETA: 4:43 - loss: 0.0978 - acc: 0.964 - ETA: 4:38 - loss: 0.0978 - acc: 0.964 - ETA: 4:33 - loss: 0.0973 - acc: 0.964 - ETA: 4:28 - loss: 0.0977 - acc: 0.964 - ETA: 4:23 - loss: 0.0983 - acc: 0.964 - ETA: 4:18 - loss: 0.0987 - acc: 0.964 - ETA: 4:13 - loss: 0.0985 - acc: 0.964 - ETA: 4:08 - loss: 0.0992 - acc: 0.964 - ETA: 4:04 - loss: 0.0998 - acc: 0.964 - ETA: 3:59 - loss: 0.0999 - acc: 0.964 - ETA: 3:54 - loss: 0.0997 - acc: 0.964 - ETA: 3:49 - loss: 0.0997 - acc: 0.964 - ETA: 3:45 - loss: 0.0997 - acc: 0.964 - ETA: 3:40 - loss: 0.1002 - acc: 0.964 - ETA: 3:35 - loss: 0.1000 - acc: 0.964 - ETA: 3:31 - loss: 0.0995 - acc: 0.964 - ETA: 3:26 - loss: 0.0992 - acc: 0.964 - ETA: 3:22 - loss: 0.0993 - acc: 0.964 - ETA: 3:17 - loss: 0.0991 - acc: 0.964 - ETA: 3:12 - loss: 0.0988 - acc: 0.964 - ETA: 3:08 - loss: 0.0993 - acc: 0.964 - ETA: 3:03 - loss: 0.0992 - acc: 0.964 - ETA: 2:59 - loss: 0.0998 - acc: 0.964 - ETA: 2:54 - loss: 0.1001 - acc: 0.964 - ETA: 2:50 - loss: 0.1002 - acc: 0.964 - ETA: 2:45 - loss: 0.1001 - acc: 0.964 - ETA: 2:41 - loss: 0.1005 - acc: 0.964 - ETA: 2:36 - loss: 0.1009 - acc: 0.963 - ETA: 2:32 - loss: 0.1012 - acc: 0.963 - ETA: 2:27 - loss: 0.1013 - acc: 0.963 - ETA: 2:23 - loss: 0.1013 - acc: 0.963 - ETA: 2:18 - loss: 0.1011 - acc: 0.963 - ETA: 2:14 - loss: 0.1011 - acc: 0.963 - ETA: 2:09 - loss: 0.1014 - acc: 0.963 - ETA: 2:05 - loss: 0.1014 - acc: 0.963 - ETA: 2:00 - loss: 0.1014 - acc: 0.963 - ETA: 1:56 - loss: 0.1016 - acc: 0.963 - ETA: 1:51 - loss: 0.1017 - acc: 0.963 - ETA: 1:47 - loss: 0.1019 - acc: 0.963 - ETA: 1:43 - loss: 0.1020 - acc: 0.963 - ETA: 1:38 - loss: 0.1021 - acc: 0.963 - ETA: 1:34 - loss: 0.1019 - acc: 0.963 - ETA: 1:29 - loss: 0.1018 - acc: 0.963 - ETA: 1:25 - loss: 0.1015 - acc: 0.963 - ETA: 1:21 - loss: 0.1016 - acc: 0.963 - ETA: 1:16 - loss: 0.1015 - acc: 0.963 - ETA: 1:12 - loss: 0.1011 - acc: 0.963 - ETA: 1:07 - loss: 0.1013 - acc: 0.963 - ETA: 1:03 - loss: 0.1013 - acc: 0.963 - ETA: 58s - loss: 0.1014 - acc: 0.963 - ETA: 54s - loss: 0.1015 - acc: 0.96 - ETA: 50s - loss: 0.1015 - acc: 0.96 - ETA: 45s - loss: 0.1015 - acc: 0.96 - ETA: 41s - loss: 0.1018 - acc: 0.96 - ETA: 36s - loss: 0.1021 - acc: 0.96 - ETA: 32s - loss: 0.1022 - acc: 0.96 - ETA: 27s - loss: 0.1023 - acc: 0.96 - ETA: 23s - loss: 0.1024 - acc: 0.96 - ETA: 19s - loss: 0.1022 - acc: 0.96 - ETA: 14s - loss: 0.1024 - acc: 0.96 - ETA: 10s - loss: 0.1024 - acc: 0.96 - ETA: 6s - loss: 0.1023 - acc: 0.9636 - ETA: 1s - loss: 0.1024 - acc: 0.963 - 514s 9ms/step - loss: 0.1025 - acc: 0.9634 - val_loss: 0.2785 - val_acc: 0.9110\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - ETA: 9:07 - loss: 0.0748 - acc: 0.974 - ETA: 8:48 - loss: 0.0834 - acc: 0.967 - ETA: 8:20 - loss: 0.0915 - acc: 0.968 - ETA: 8:02 - loss: 0.0916 - acc: 0.968 - ETA: 7:49 - loss: 0.0901 - acc: 0.968 - ETA: 7:38 - loss: 0.0903 - acc: 0.968 - ETA: 7:28 - loss: 0.0858 - acc: 0.970 - ETA: 7:23 - loss: 0.0882 - acc: 0.971 - ETA: 7:16 - loss: 0.0910 - acc: 0.971 - ETA: 7:10 - loss: 0.0911 - acc: 0.971 - ETA: 7:04 - loss: 0.0911 - acc: 0.971 - ETA: 6:59 - loss: 0.0889 - acc: 0.971 - ETA: 6:53 - loss: 0.0898 - acc: 0.971 - ETA: 6:48 - loss: 0.0905 - acc: 0.971 - ETA: 6:42 - loss: 0.0886 - acc: 0.972 - ETA: 6:38 - loss: 0.0895 - acc: 0.971 - ETA: 6:33 - loss: 0.0872 - acc: 0.972 - ETA: 6:28 - loss: 0.0868 - acc: 0.972 - ETA: 6:24 - loss: 0.0861 - acc: 0.972 - ETA: 6:19 - loss: 0.0848 - acc: 0.973 - ETA: 6:14 - loss: 0.0847 - acc: 0.973 - ETA: 6:10 - loss: 0.0844 - acc: 0.972 - ETA: 6:06 - loss: 0.0854 - acc: 0.972 - ETA: 6:01 - loss: 0.0858 - acc: 0.972 - ETA: 5:56 - loss: 0.0863 - acc: 0.972 - ETA: 5:54 - loss: 0.0875 - acc: 0.971 - ETA: 5:49 - loss: 0.0875 - acc: 0.971 - ETA: 5:44 - loss: 0.0865 - acc: 0.972 - ETA: 5:40 - loss: 0.0864 - acc: 0.972 - ETA: 5:36 - loss: 0.0884 - acc: 0.971 - ETA: 5:31 - loss: 0.0879 - acc: 0.972 - ETA: 5:27 - loss: 0.0874 - acc: 0.972 - ETA: 5:22 - loss: 0.0872 - acc: 0.972 - ETA: 5:18 - loss: 0.0879 - acc: 0.971 - ETA: 5:15 - loss: 0.0885 - acc: 0.971 - ETA: 5:13 - loss: 0.0882 - acc: 0.971 - ETA: 5:09 - loss: 0.0881 - acc: 0.971 - ETA: 5:05 - loss: 0.0879 - acc: 0.970 - ETA: 5:01 - loss: 0.0890 - acc: 0.970 - ETA: 4:56 - loss: 0.0884 - acc: 0.970 - ETA: 4:52 - loss: 0.0880 - acc: 0.971 - ETA: 4:47 - loss: 0.0876 - acc: 0.971 - ETA: 4:43 - loss: 0.0887 - acc: 0.971 - ETA: 4:38 - loss: 0.0884 - acc: 0.971 - ETA: 4:34 - loss: 0.0892 - acc: 0.971 - ETA: 4:29 - loss: 0.0893 - acc: 0.971 - ETA: 4:25 - loss: 0.0904 - acc: 0.970 - ETA: 4:21 - loss: 0.0898 - acc: 0.970 - ETA: 4:16 - loss: 0.0904 - acc: 0.970 - ETA: 4:12 - loss: 0.0905 - acc: 0.970 - ETA: 4:08 - loss: 0.0902 - acc: 0.970 - ETA: 4:03 - loss: 0.0902 - acc: 0.970 - ETA: 3:59 - loss: 0.0906 - acc: 0.970 - ETA: 3:55 - loss: 0.0905 - acc: 0.970 - ETA: 3:51 - loss: 0.0909 - acc: 0.969 - ETA: 3:46 - loss: 0.0916 - acc: 0.969 - ETA: 3:42 - loss: 0.0922 - acc: 0.969 - ETA: 3:38 - loss: 0.0924 - acc: 0.969 - ETA: 3:33 - loss: 0.0926 - acc: 0.968 - ETA: 3:29 - loss: 0.0933 - acc: 0.968 - ETA: 3:25 - loss: 0.0932 - acc: 0.968 - ETA: 3:20 - loss: 0.0935 - acc: 0.968 - ETA: 3:16 - loss: 0.0935 - acc: 0.968 - ETA: 3:12 - loss: 0.0939 - acc: 0.968 - ETA: 3:08 - loss: 0.0939 - acc: 0.968 - ETA: 3:03 - loss: 0.0941 - acc: 0.968 - ETA: 3:00 - loss: 0.0943 - acc: 0.968 - ETA: 2:55 - loss: 0.0943 - acc: 0.968 - ETA: 2:51 - loss: 0.0942 - acc: 0.967 - ETA: 2:47 - loss: 0.0943 - acc: 0.967 - ETA: 2:42 - loss: 0.0942 - acc: 0.968 - ETA: 2:38 - loss: 0.0941 - acc: 0.968 - ETA: 2:34 - loss: 0.0942 - acc: 0.968 - ETA: 2:30 - loss: 0.0943 - acc: 0.968 - ETA: 2:25 - loss: 0.0947 - acc: 0.967 - ETA: 2:21 - loss: 0.0942 - acc: 0.968 - ETA: 2:17 - loss: 0.0938 - acc: 0.968 - ETA: 2:12 - loss: 0.0939 - acc: 0.968 - ETA: 2:08 - loss: 0.0937 - acc: 0.968 - ETA: 2:04 - loss: 0.0939 - acc: 0.968 - ETA: 2:00 - loss: 0.0943 - acc: 0.968 - ETA: 1:55 - loss: 0.0948 - acc: 0.967 - ETA: 1:51 - loss: 0.0948 - acc: 0.967 - ETA: 1:47 - loss: 0.0949 - acc: 0.967 - ETA: 1:43 - loss: 0.0950 - acc: 0.967 - ETA: 1:38 - loss: 0.0950 - acc: 0.967 - ETA: 1:34 - loss: 0.0950 - acc: 0.967 - ETA: 1:30 - loss: 0.0953 - acc: 0.967 - ETA: 1:26 - loss: 0.0955 - acc: 0.967 - ETA: 1:21 - loss: 0.0960 - acc: 0.967 - ETA: 1:17 - loss: 0.0960 - acc: 0.967 - ETA: 1:13 - loss: 0.0960 - acc: 0.967 - ETA: 1:09 - loss: 0.0961 - acc: 0.967 - ETA: 1:04 - loss: 0.0964 - acc: 0.966 - ETA: 1:00 - loss: 0.0967 - acc: 0.966 - ETA: 56s - loss: 0.0968 - acc: 0.966 - ETA: 52s - loss: 0.0970 - acc: 0.96 - ETA: 48s - loss: 0.0969 - acc: 0.96 - ETA: 43s - loss: 0.0972 - acc: 0.96 - ETA: 39s - loss: 0.0970 - acc: 0.96 - ETA: 35s - loss: 0.0969 - acc: 0.96 - ETA: 31s - loss: 0.0969 - acc: 0.96 - ETA: 26s - loss: 0.0968 - acc: 0.96 - ETA: 22s - loss: 0.0968 - acc: 0.96 - ETA: 18s - loss: 0.0967 - acc: 0.96 - ETA: 14s - loss: 0.0970 - acc: 0.96 - ETA: 10s - loss: 0.0969 - acc: 0.96 - ETA: 5s - loss: 0.0970 - acc: 0.9662 - ETA: 1s - loss: 0.0970 - acc: 0.966 - 492s 9ms/step - loss: 0.0972 - acc: 0.9661 - val_loss: 0.2968 - val_acc: 0.9091\n",
      "Epoch 10/10\n",
      "56000/56000 [==============================] - ETA: 8:12 - loss: 0.0825 - acc: 0.960 - ETA: 7:58 - loss: 0.0793 - acc: 0.970 - ETA: 7:39 - loss: 0.0761 - acc: 0.973 - ETA: 7:25 - loss: 0.0775 - acc: 0.972 - ETA: 7:17 - loss: 0.0773 - acc: 0.972 - ETA: 7:13 - loss: 0.0790 - acc: 0.970 - ETA: 7:07 - loss: 0.0822 - acc: 0.969 - ETA: 7:02 - loss: 0.0849 - acc: 0.970 - ETA: 6:58 - loss: 0.0830 - acc: 0.970 - ETA: 6:53 - loss: 0.0831 - acc: 0.971 - ETA: 6:49 - loss: 0.0819 - acc: 0.971 - ETA: 6:44 - loss: 0.0815 - acc: 0.972 - ETA: 6:40 - loss: 0.0832 - acc: 0.971 - ETA: 6:35 - loss: 0.0828 - acc: 0.972 - ETA: 6:31 - loss: 0.0830 - acc: 0.972 - ETA: 6:27 - loss: 0.0823 - acc: 0.972 - ETA: 6:23 - loss: 0.0815 - acc: 0.973 - ETA: 6:19 - loss: 0.0825 - acc: 0.973 - ETA: 6:15 - loss: 0.0838 - acc: 0.972 - ETA: 6:10 - loss: 0.0847 - acc: 0.972 - ETA: 6:06 - loss: 0.0849 - acc: 0.972 - ETA: 6:02 - loss: 0.0857 - acc: 0.971 - ETA: 5:58 - loss: 0.0860 - acc: 0.971 - ETA: 5:55 - loss: 0.0859 - acc: 0.971 - ETA: 5:51 - loss: 0.0857 - acc: 0.971 - ETA: 5:46 - loss: 0.0858 - acc: 0.971 - ETA: 5:42 - loss: 0.0858 - acc: 0.971 - ETA: 5:38 - loss: 0.0855 - acc: 0.971 - ETA: 5:33 - loss: 0.0850 - acc: 0.971 - ETA: 5:29 - loss: 0.0840 - acc: 0.971 - ETA: 5:25 - loss: 0.0830 - acc: 0.971 - ETA: 5:21 - loss: 0.0836 - acc: 0.971 - ETA: 5:16 - loss: 0.0846 - acc: 0.971 - ETA: 5:11 - loss: 0.0851 - acc: 0.970 - ETA: 5:07 - loss: 0.0859 - acc: 0.970 - ETA: 5:03 - loss: 0.0869 - acc: 0.969 - ETA: 4:59 - loss: 0.0868 - acc: 0.969 - ETA: 4:55 - loss: 0.0865 - acc: 0.970 - ETA: 4:51 - loss: 0.0869 - acc: 0.970 - ETA: 4:47 - loss: 0.0870 - acc: 0.970 - ETA: 4:43 - loss: 0.0875 - acc: 0.969 - ETA: 4:39 - loss: 0.0877 - acc: 0.969 - ETA: 4:35 - loss: 0.0871 - acc: 0.969 - ETA: 4:31 - loss: 0.0871 - acc: 0.969 - ETA: 4:27 - loss: 0.0869 - acc: 0.969 - ETA: 4:22 - loss: 0.0867 - acc: 0.969 - ETA: 4:18 - loss: 0.0867 - acc: 0.970 - ETA: 4:14 - loss: 0.0869 - acc: 0.970 - ETA: 4:10 - loss: 0.0867 - acc: 0.970 - ETA: 4:06 - loss: 0.0874 - acc: 0.969 - ETA: 4:02 - loss: 0.0878 - acc: 0.969 - ETA: 3:57 - loss: 0.0886 - acc: 0.969 - ETA: 3:53 - loss: 0.0885 - acc: 0.969 - ETA: 3:49 - loss: 0.0884 - acc: 0.969 - ETA: 3:45 - loss: 0.0887 - acc: 0.969 - ETA: 3:41 - loss: 0.0885 - acc: 0.969 - ETA: 3:36 - loss: 0.0898 - acc: 0.969 - ETA: 3:32 - loss: 0.0896 - acc: 0.968 - ETA: 3:28 - loss: 0.0900 - acc: 0.968 - ETA: 3:24 - loss: 0.0899 - acc: 0.968 - ETA: 3:20 - loss: 0.0898 - acc: 0.968 - ETA: 3:15 - loss: 0.0900 - acc: 0.968 - ETA: 3:11 - loss: 0.0908 - acc: 0.968 - ETA: 3:07 - loss: 0.0910 - acc: 0.968 - ETA: 3:03 - loss: 0.0910 - acc: 0.968 - ETA: 2:59 - loss: 0.0907 - acc: 0.968 - ETA: 2:55 - loss: 0.0911 - acc: 0.968 - ETA: 2:51 - loss: 0.0910 - acc: 0.968 - ETA: 2:46 - loss: 0.0912 - acc: 0.968 - ETA: 2:42 - loss: 0.0914 - acc: 0.968 - ETA: 2:38 - loss: 0.0916 - acc: 0.968 - ETA: 2:34 - loss: 0.0916 - acc: 0.967 - ETA: 2:30 - loss: 0.0911 - acc: 0.968 - ETA: 2:26 - loss: 0.0908 - acc: 0.968 - ETA: 2:22 - loss: 0.0906 - acc: 0.968 - ETA: 2:18 - loss: 0.0908 - acc: 0.968 - ETA: 2:13 - loss: 0.0907 - acc: 0.968 - ETA: 2:09 - loss: 0.0908 - acc: 0.968 - ETA: 2:05 - loss: 0.0909 - acc: 0.968 - ETA: 2:01 - loss: 0.0910 - acc: 0.968 - ETA: 1:57 - loss: 0.0911 - acc: 0.968 - ETA: 1:53 - loss: 0.0913 - acc: 0.968 - ETA: 1:48 - loss: 0.0913 - acc: 0.968 - ETA: 1:44 - loss: 0.0912 - acc: 0.968 - ETA: 1:40 - loss: 0.0909 - acc: 0.968 - ETA: 1:36 - loss: 0.0909 - acc: 0.968 - ETA: 1:32 - loss: 0.0910 - acc: 0.968 - ETA: 1:28 - loss: 0.0912 - acc: 0.968 - ETA: 1:24 - loss: 0.0912 - acc: 0.968 - ETA: 1:19 - loss: 0.0912 - acc: 0.968 - ETA: 1:15 - loss: 0.0914 - acc: 0.968 - ETA: 1:11 - loss: 0.0914 - acc: 0.968 - ETA: 1:07 - loss: 0.0912 - acc: 0.968 - ETA: 1:03 - loss: 0.0914 - acc: 0.968 - ETA: 59s - loss: 0.0915 - acc: 0.968 - ETA: 55s - loss: 0.0918 - acc: 0.96 - ETA: 51s - loss: 0.0917 - acc: 0.96 - ETA: 46s - loss: 0.0919 - acc: 0.96 - ETA: 42s - loss: 0.0922 - acc: 0.96 - ETA: 38s - loss: 0.0926 - acc: 0.96 - ETA: 34s - loss: 0.0926 - acc: 0.96 - ETA: 30s - loss: 0.0924 - acc: 0.96 - ETA: 26s - loss: 0.0927 - acc: 0.96 - ETA: 22s - loss: 0.0926 - acc: 0.96 - ETA: 18s - loss: 0.0924 - acc: 0.96 - ETA: 13s - loss: 0.0924 - acc: 0.96 - ETA: 9s - loss: 0.0926 - acc: 0.9675 - ETA: 5s - loss: 0.0926 - acc: 0.967 - ETA: 1s - loss: 0.0924 - acc: 0.967 - 483s 9ms/step - loss: 0.0924 - acc: 0.9675 - val_loss: 0.2938 - val_acc: 0.9084\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "# Initialising the model\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Adding embedding\n",
    "model_2.add(Embedding(len(vocabulary), embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "# Adding first LSTM layer\n",
    "model_2.add(LSTM(100,return_sequences=True, dropout=0.4, recurrent_dropout=0.4))\n",
    "\n",
    "# Adding second LSTM layer\n",
    "model_2.add(LSTM(100, dropout=0.4, recurrent_dropout=0.4))\n",
    "\n",
    "# Adding output layer\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Printing the model summary\n",
    "print(model_2.summary())\n",
    "\n",
    "# Compiling the model\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the data to the model\n",
    "history_2 = model_2.fit(X_train, Y_train, nb_epoch=10, batch_size=512 ,verbose=1,validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LuaAL-XipkF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.84%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5b6b663c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_2.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Test and train accuracy of the model\n",
    "model_2_test = scores[1]\n",
    "model_2_train = max(history_2.history['acc'])\n",
    "\n",
    "# Plotting Train and Test Loss VS no. of epochs\n",
    "# list of epoch numbers\n",
    "x = list(range(1,11))\n",
    "\n",
    "# Validation loss\n",
    "vy = history_2.history['val_loss']\n",
    "# Training loss\n",
    "ty = history_2.history['loss']\n",
    "\n",
    "# Calling the function to draw the plot\n",
    "plt_dynamic(x, vy, ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGEwuVOlkV0P"
   },
   "source": [
    "<h2> [3] Conclusions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mA-vsN3vk6Kp"
   },
   "source": [
    "<h2>(a) Table (Different models with their train and test accuracies):</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qH-atzoLlJyK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prettytable in c:\\users\\dell\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+--------------------+--------------------+\n",
      "| S.NO. |         MODEL          | Training Accuracy  |   Test Accuracy    |\n",
      "+-------+------------------------+--------------------+--------------------+\n",
      "|   1   | RNN With 1 LSTM Layer  |       0.977        | 0.9026428571428572 |\n",
      "|   2   | RNN With 2 LSTM Layers | 0.9675000000681196 | 0.9084285714285715 |\n",
      "+-------+------------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# Installing the library prettytable\n",
    "!pip install prettytable\n",
    "\n",
    "# Creating table using PrettyTable library\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Names of models\n",
    "names = ['RNN With 1 LSTM Layer','RNN With 2 LSTM Layers']\n",
    "\n",
    "# Training accuracies\n",
    "train_acc = [model_1_train,model_2_train]\n",
    "\n",
    "# Test accuracies\n",
    "test_acc = [model_1_test,model_2_test]\n",
    "\n",
    "numbering = [1,2]\n",
    "\n",
    "# Initializing prettytable\n",
    "ptable = PrettyTable()\n",
    "\n",
    "# Adding columns\n",
    "ptable.add_column(\"S.NO.\",numbering)\n",
    "ptable.add_column(\"MODEL\",names)\n",
    "ptable.add_column(\"Training Accuracy\",train_acc)\n",
    "ptable.add_column(\"Test Accuracy\",test_acc)\n",
    "\n",
    "# Printing the Table\n",
    "print(ptable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9S36XTvmAvX"
   },
   "source": [
    "\n",
    "<h2>(b). Procedure Followed :</h2>\n",
    "\n",
    "Step 1: Load Amazon Fine Food Reviews dataset\n",
    "\n",
    "Step 2: Perform text pre-processing\n",
    "\n",
    "Step 3: Sort the dataset on the basis of time and after that find vocabulary for all the reviews in the dataset\n",
    "    \n",
    "Step 4: Now compute frequencies for each word of vocabulary\n",
    "    \n",
    "Step 5: Index each word in the decreasing order of frequencies (Word with max frequency will have rank 1 or index 1)\n",
    "\n",
    "Step 6: Convert the dataset into imdb dataset format\n",
    "    \n",
    "Step 7: Split whole dataset for training set and test set\n",
    "    \n",
    "Step 8: Now pad or truncate each review intpo sequences of length 100\n",
    "    \n",
    "Step 9: Now implement RNN with 1 and 2 LSTM layers\n",
    "    \n",
    "Step 10: Finding accuracy for each\n",
    "    \n",
    "Step 11: Drawing Binary Crossentropy Loss VS No.of Epochs plot"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_IMDB.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
